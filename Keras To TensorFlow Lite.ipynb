{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python -V\n",
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make sure the tensorflow version is large than 1.8\n",
    "%%bash\n",
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# example to conver weight\n",
    "FILE=flower_gpu\n",
    "tflite_convert \\\n",
    "  --output_file='./weight/mnist.tflite' \\\n",
    "  --keras_model_file='./weight/mnist.h5'\n",
    "  \n",
    "python keras_to_tensorflow.py --input_model=weight/${FILE}.h5 --output_model=weight/${FILE}.pb\n",
    "# toco --graph_def_file='./weight/mnist.pb' --output_file='./weight/mnist.tflite' --input_shape='1,28,28,1' --input_arrays='input' --output_arrays='output/Softmax' --output_format=TFLITE  --input_format=TENSORFLOW_GRAPHDEF \n",
    "# at=TENSORFLOW_GRAPHDEF  -inference_type=FlOAT\n",
    "\n",
    "#should change  Input_layer and Output_layer name to show in keras_to_tensorflow\n",
    "FILE=cifa \n",
    "IMAGE_SIZE=224 \\\n",
    "INPUT=input_2 \\\n",
    "OUTPUT=predictions/Softmax\n",
    "toco \\\n",
    "  --graph_def_file=weight/${FILE}.pb \\\n",
    "  --output_file=weight/${FILE}.lite \\\n",
    "  --input_format=TENSORFLOW_GRAPHDEF \\\n",
    "  --output_format=TFLITE \\\n",
    "  --input_shape=1,${IMAGE_SIZE},${IMAGE_SIZE},3 \\\n",
    "  --input_array=${INPUT} \\\n",
    "  --output_array=${OUTPUT} \\\n",
    "  --inference_type=FLOAT \\\n",
    "  --input_data_type=FLOAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a mnist and save .tflite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    " \n",
    "mnist = input_data.read_data_sets(\"mnist\",one_hot=True)\n",
    " \n",
    " \n",
    "# 定义批次大小\n",
    "batch_size = 128\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "print(n_batch)\n",
    " \n",
    "# 定义placeholder\n",
    "x = tf.placeholder(tf.float32,[1,784],name='input_x')\n",
    "y = tf.placeholder(tf.float32,[1,10],name='output_y')\n",
    " \n",
    "# 定义 测试\n",
    "x_test = tf.placeholder(tf.float32,[None,784],name='input_test_x')\n",
    "y_test = tf.placeholder(tf.float32,[None,10],name='input_test_y')\n",
    " \n",
    "# 创建一个简单的神经网络\n",
    "W = tf.Variable(tf.zeros([784,10]),name=\"W\")\n",
    "b = tf.Variable(tf.zeros([1,10]),name=\"b\")\n",
    " \n",
    "prediction = tf.nn.softmax(tf.matmul(x,W)+b)\n",
    " \n",
    " \n",
    " \n",
    "# 创建损失函数\n",
    "train = tf.train.GradientDescentOptimizer(0.02).minimize(tf.reduce_mean(tf.square(y-prediction)))\n",
    " \n",
    "# 名称转换\n",
    "def canonical_name(x):\n",
    "  return x.name.split(\":\")[0]\n",
    " \n",
    "# 计算准确率\n",
    "test_prediction = tf.nn.softmax(tf.matmul(x_test,W)+b)\n",
    "accuarcy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_test,1),tf.argmax(test_prediction,1)),tf.float32))\n",
    " \n",
    "init = tf.global_variables_initializer()\n",
    "out = tf.identity(prediction, name=\"output\")\n",
    " \n",
    "print('Session')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(1):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "            for index in range(len(batch_xs)):\n",
    "                xs = batch_xs[index].reshape(1,784)\n",
    "                ys = batch_ys[index].reshape(1,10)\n",
    "                sess.run(train, feed_dict={x: xs, y: ys})\n",
    "            acc = sess.run(accuarcy,feed_dict={x_test:mnist.test.images,y_test:mnist.test.labels})\n",
    "            print(\"over\"+str(acc))\n",
    " \n",
    "    frozen_tensors = [out]\n",
    "    out_tensors = [out]\n",
    " \n",
    "    frozen_graphdef = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, list(map(canonical_name, frozen_tensors)))\n",
    "    tflite_model = tf.contrib.lite.toco_convert(frozen_graphdef, [x], out_tensors)\n",
    " \n",
    "    open(\"writer_model.tflite\", \"wb\").write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import plot_model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print(y_train.shape, y_train[0])\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "input = Input(name ='input', shape = input_shape)\n",
    "x = Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu')(input)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(num_classes, activation='softmax', name = 'output')(x)\n",
    "model = Model(inputs = input, outputs = x)\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "model.summary()\n",
    "model.save('./weight/mnist.h5')\n",
    "with open('./weight/mnist.json', 'w+') as f:\n",
    "    f.write(model.to_json())\n",
    "plot_model(model, to_file='./weight/mnist.png', show_shapes = True)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "os.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifar Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "img_width, img_height = 224, 224\n",
    "train_data_dir = '/home/sober/ftp_bk/Dataset/Tmp_Train/cifar/Train/'\n",
    "val_data_dir = '/home/sober/ftp_bk/Dataset/Tmp_Train/cifar/Val/'\n",
    "batch_size = 8\n",
    "epochs = 200\n",
    "nb_train_samples, nb_validation_samples = 5000,  1000\n",
    "\n",
    "\n",
    "def mean(x):\n",
    "    return (x-128.0)/128.0\n",
    "\n",
    "model = ResNet50(include_top = True, classes = 10, weights=None, input_shape=(img_width, img_height, 3))\n",
    "# model.summary()\n",
    "# model.layers.pop()\n",
    "# output = model.layers[-1].output\n",
    "# output = Flatten()(output)\n",
    "# output = Dense(10, activation='softmax')(output)\n",
    "# model = Model(model.input, output)\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    \n",
    "train_datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "    vertical_flip=True, rotation_range=15, preprocessing_function=mean)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=mean)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "            train_data_dir, shuffle=True,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=batch_size, class_mode='categorical')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "            val_data_dir, shuffle=True,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           verbose=1,\n",
    "#           validation_data=(x_test, y_test))\n",
    "# EarlyStopping(monitor='val_acc', patience=5,\n",
    "#                                       verbose=0,\n",
    "#                                       mode='auto'),\n",
    "# ReduceLROnPlateau(monitor='val_acc', factor=0.2,\n",
    "#                                               patience=10, verbose=0,\n",
    "#                                               cooldown=1, epsilon=0.001)\n",
    "\n",
    "callbacks = []\n",
    "historys = model.fit_generator(callbacks=callbacks,\n",
    "                                                use_multiprocessing=True,\n",
    "                                                workers=5, max_queue_size=20,\n",
    "                                                generator=train_generator,\n",
    "                                                steps_per_epoch=nb_train_samples // batch_size,\n",
    "                                                epochs=epochs,\n",
    "                                                validation_steps=nb_validation_samples // batch_size,\n",
    "                                                validation_data=validation_generator)\n",
    "# model.summary()\n",
    "model.save('./weight/cifa.h5')\n",
    "with open('./weight/cifa.json', 'w+') as f:\n",
    "    f.write(model.to_json())\n",
    "plot_model(model, to_file='./weight/cifa.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.models import model_from_json\n",
    "from os import listdir\n",
    "from LoadWriteModel import loadWriteModel\n",
    "from os.path import join, isdir\n",
    "import gc\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "\n",
    "class MyPredict(object):\n",
    "    # directory\n",
    "    CATEGORY = 20\n",
    "    # file\n",
    "    NORMAL = 10\n",
    "\n",
    "    def __init__(self, model, labels=None, image_shape=(224, 224), level=None,\n",
    "                 mother_path='./Predict'):\n",
    "        \"\"\"\n",
    "        :param model:\n",
    "        :param labels:\n",
    "        :param image_shape:\n",
    "        :param level:   [console level] or ['console level', 'log level']\n",
    "        logging.INFO\n",
    "        :param mother_path:     log and predict summary image save file path\n",
    "        \"\"\"\n",
    "\n",
    "        self.mother_path = mother_path\n",
    "        self.labels = labels\n",
    "        self.image_shape = image_shape\n",
    "        self.__init_default_variable()\n",
    "        self.__init_environment()\n",
    "        self.__init_log(\n",
    "            level=level if level is not None else self.default_level)\n",
    "        self.model = model\n",
    "        logging.info('Init finish')\n",
    "\n",
    "    def __init_default_variable(self):\n",
    "        \"\"\"\n",
    "        the default variable and path\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.default_logname = 'MyPredict_log.log'\n",
    "        self.default_level = [logging.DEBUG]\n",
    "        self.default_pred_filename_ext = '_predict.csv'\n",
    "        self.default_confusion_matrix_path = join(self.mother_path,\n",
    "                                                  'confusion_matrix.csv')\n",
    "        self.default_imagefile_extension = 'jpg|jpeg|bmp|png'\n",
    "\n",
    "    def __init_environment(self):\n",
    "        # log remove\n",
    "        for root, _, files in os.walk('./'):\n",
    "            for f in files:\n",
    "                if f.endswith(r'.log'):\n",
    "                    os.remove(join(root, f))\n",
    "\n",
    "        # image save path clean\n",
    "        if os.path.exists(self.mother_path):\n",
    "            shutil.rmtree(self.mother_path)\n",
    "\n",
    "        # create mother folder\n",
    "        os.makedirs(self.mother_path)\n",
    "\n",
    "    def __init_log(self, level):\n",
    "        \"\"\"\n",
    "        :param level:  level[0] is log file output level, level[1] is console output level\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if len(level) == 1:\n",
    "            if type(level) is list:\n",
    "                level = level[0]\n",
    "            logging.basicConfig(level=level)\n",
    "        elif len(level) == 2:\n",
    "            logging.basicConfig(filename=self.default_logname, level=level[1])\n",
    "            console = logging.StreamHandler()\n",
    "            console.setLevel(level[0])\n",
    "            logging.getLogger('').addHandler(console)\n",
    "        else:\n",
    "            raise RuntimeError(\"The log level's length must be 1 or 2\")\n",
    "        logging.info('Log init')\n",
    "\n",
    "    # def set_output_level(self, level):\n",
    "    #     size = len(level)\n",
    "    #     assert (size == 1 or size == 2),  'Level length must be 0 or 1'\n",
    "    #     if size == 1:\n",
    "    #         level = level[0] if type(level) is list\n",
    "    #         logging.Logger.setLevel(level)\n",
    "    #     else:\n",
    "    #         console.setLevel(level[1])\n",
    "    #         log\n",
    "\n",
    "    def set_labels(self, labels):\n",
    "        \"\"\"\n",
    "        :param labels:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.labels is None or self.labels == '':\n",
    "            logging.info(\"Label Change\\n Origin: %s\\n Now: %s\\n\", self.labels,\n",
    "                         labels)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __del__(self):\n",
    "        self.label = None\n",
    "        del self.model\n",
    "        self.model = None\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    def __init_csv(self):\n",
    "        \"\"\"\n",
    "        init the log file\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        columns_name = ['file', 'r_label', 'r_index', 't_index', 't_label',\n",
    "                        'timestamp', 'image', 'check']\n",
    "        for index in self.labels:\n",
    "            columns_name.append('l_' + index)\n",
    "        columns_name = sorted(columns_name)\n",
    "        self.csv = pd.DataFrame(columns=columns_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_log_model(name, history_path, history_load=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param name:\n",
    "        :param history_path:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        read from history file and generate model name if not given\n",
    "        :param name:    model name which need to be load\n",
    "        :param history: history path\n",
    "        :param model:   model\n",
    "        :return:    loaded model\n",
    "        \"\"\"\n",
    "        assert history_path != None and os.path.exists(\n",
    "            history_path), 'History path ' \\\n",
    "                           'must be valid'\n",
    "        assert name != None\n",
    "        loadboostrap = loadWriteModel(history_path)\n",
    "        model, _ = loadboostrap.load(name, weight_load=True,\n",
    "                                     history_load=history_load)\n",
    "        model.name = name\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def read_file_model(model_path, weight_path, show_summary=False):\n",
    "        \"\"\" read model and load weight from json and h5 file\n",
    "\n",
    "        model_path = './Result/model/i_shift.json'\n",
    "        weight_path = './Result/weight/i_shift.h5'\n",
    "        model = MyPredict.read_file_model(model_path, weight_path, show_summary=True)\n",
    "\n",
    "        :param model_path:\n",
    "        :param weight_path:\n",
    "        :param show_summary:    show model summary or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        assert (os.path.exists(model_path) is True and model_path.endswith(\n",
    "            '.json')), 'Model Path invalid'\n",
    "        assert (os.path.exists(weight_path) is True and weight_path.endswith(\n",
    "            '.h5')), 'Weight Path invalid'\n",
    "        with open(model_path, 'r') as f:\n",
    "            model = model_from_json(f.read())\n",
    "        model.load_weights(weight_path, by_name=True)\n",
    "        end = time.time()\n",
    "        print('Model Loaded Spend:%0.3fs' % (end - start))\n",
    "        if show_summary:\n",
    "            model.summary()\n",
    "\n",
    "        if model.name is None:\n",
    "            model.name = MyPredict.generate_name()\n",
    "        return model\n",
    "\n",
    "    # def __read_model(self, name=None, history=None, model=None):\n",
    "    #     \"\"\"\n",
    "    #     read from history file and generate model name if not given\n",
    "    #     :param name:    model name which need to be load\n",
    "    #     :param history: history path\n",
    "    #     :param model:   model\n",
    "    #     :return:    loaded model\n",
    "    #     \"\"\"\n",
    "    #     if model != None:\n",
    "    #         self.model_name = self.__generate_name()\n",
    "    #         assert model != None\n",
    "    #         return model\n",
    "    #     else:\n",
    "    #         assert history != None\n",
    "    #         assert name != None\n",
    "    #         self.model_name = name\n",
    "    #         # model load\n",
    "    #         loadboostrap = loadWriteModel(history)\n",
    "    #         model, _ = loadboostrap.load(name, weight_load=True)\n",
    "    #         return model\n",
    "\n",
    "    def dir_paths(self, path):\n",
    "        \"\"\"\n",
    "        get the validate directory full path from val_path\n",
    "        :param path:\n",
    "        :return: directory name and directory path, which has been sorted\n",
    "        \"\"\"\n",
    "        dir_array = []\n",
    "        for f in listdir(path):\n",
    "            total_path = join(path, f)\n",
    "            if isdir(total_path):\n",
    "                dir_array.append(total_path)\n",
    "        dir_path_array = sorted(dir_array)\n",
    "        labels = [os.path.split(d)[-1] for d in dir_path_array]\n",
    "        return labels, dir_path_array\n",
    "\n",
    "    def __generate_array(self, path):\n",
    "        \"\"\"\n",
    "        read each image file and save them to a numpy array\n",
    "        :param path:\n",
    "        :return: image_arrays, predict_list\n",
    "        \"\"\"\n",
    "        image_arrays = None\n",
    "        predict_list = []\n",
    "        if os.path.isdir(path):\n",
    "            file_list = [join(path, f) for f in listdir(path)]\n",
    "        else:\n",
    "            file_list = [path]\n",
    "\n",
    "        for absolute_path in file_list:\n",
    "            if self.__check_extension(absolute_path):\n",
    "                predict_list.append(absolute_path)\n",
    "                image = load_img(absolute_path)\n",
    "                image = image.resize(self.image_shape)\n",
    "                # load all image to numpy and convert to (None, 224, 224, 3)\n",
    "                image = img_to_array(image)\n",
    "                image = image[np.newaxis, :]\n",
    "                try:\n",
    "                    image_arrays = np.concatenate((image_arrays, image), 0)\n",
    "                except:\n",
    "                    image_arrays = image\n",
    "\n",
    "        if len(predict_list) == 0:\n",
    "            raise RuntimeError(\n",
    "                \"No image find in predit folder path, Please check the file \"\n",
    "                \"extension is \" + self.default_imagefile_extension)\n",
    "\n",
    "        logging.info('Stack image array size: %s', image_arrays.shape)\n",
    "        return image_arrays, predict_list\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_name():\n",
    "        now = datetime.datetime.now()\n",
    "        timestamp = now.strftime('%Y%m%d_%H_%M_%S')\n",
    "        name = timestamp + '_%02d' % (np.random.randint(100))\n",
    "        return name\n",
    "\n",
    "    def __predict(self, image_arrays, predict_list, target_index,\n",
    "                  save_image=True):\n",
    "        \"\"\"\n",
    "        predict, and get result which is a softmax array, and get the max result to compare\n",
    "            the target one\n",
    "        :param predict_list:    the predict file list\n",
    "        :param target_index:    the image which belong to his category\n",
    "        :param save_image:      save image which is predict wrong\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        result = self.model.predict(image_arrays)\n",
    "\n",
    "        # result summary csv row\n",
    "        for i in range(len(result)):\n",
    "            real_index = np.argmax(result[i])\n",
    "            row = {}\n",
    "            wrong_file = predict_list[i]\n",
    "            row['file'] = wrong_file\n",
    "            row['t_index'] = int(target_index)\n",
    "            row['t_label'] = self.labels[target_index]\n",
    "            row['r_index'] = int(real_index)\n",
    "            row['r_label'] = self.labels[real_index]\n",
    "            name = MyPredict.generate_name()\n",
    "            row['timestamp'] = name[:-3]\n",
    "            for index, label in enumerate(self.labels):\n",
    "                key = 'l_' + label\n",
    "                row[key] = str(np.round(result[i][index], 3))\n",
    "\n",
    "            if real_index != target_index:\n",
    "                # predict wrong\n",
    "                row['check'] = 0\n",
    "                logging.debug('Predict:\\n%s', row)\n",
    "\n",
    "                # TODO 保存文件,用统一Flag\n",
    "                if save_image:\n",
    "                    row['image'] = self.__save_image(image_arrays[i], result[i],\n",
    "                                                     wrong_file, name)\n",
    "            else:\n",
    "                row['check'] = 1\n",
    "\n",
    "            self.csv = self.csv.append(row, ignore_index=True)\n",
    "\n",
    "        logging.info('One Folder image predict finish')\n",
    "\n",
    "    def __save_image(self, img_array, result, title, file_name):\n",
    "        \"\"\"\n",
    "        save image\n",
    "        :param img_array:   image array\n",
    "        :param result:      softmax rate\n",
    "        :param title:       image title\n",
    "        :param file_name:   image file name without extension\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        def auto_label(rects, aix):\n",
    "            # Get y-axis height to calculate label position from.\n",
    "            (y_bottom, y_top) = aix.get_ylim()\n",
    "            y_height = y_top - y_bottom\n",
    "\n",
    "            for rect in rects:\n",
    "                height = rect.get_height()\n",
    "\n",
    "                # Fraction of axis height taken up by this rectangle\n",
    "                p_height = (height / y_height)\n",
    "\n",
    "                # If we can fit the label above the column, do that;\n",
    "                # otherwise, put it inside the column.\n",
    "                if p_height > 0.95:  # arbitrary; 95% looked good to me.\n",
    "                    label_position = height - (y_height * 0.05)\n",
    "                else:\n",
    "                    label_position = height + (y_height * 0.01)\n",
    "                height = '%s' % (np.round(height, 2))\n",
    "\n",
    "                aix.text(rect.get_x() + rect.get_width() / 2., label_position,\n",
    "                         height, ha='center', va='bottom', fontsize=6)\n",
    "\n",
    "\n",
    "        labels = self.labels\n",
    "\n",
    "        plt.figure()\n",
    "        image = array_to_img(img_array)\n",
    "        plt.subplot(211)\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)\n",
    "\n",
    "        # TODO 每个bar都有不同颜色\n",
    "        ax = plt.subplot(212)\n",
    "        label_size = len(result)\n",
    "        xticks = np.arange(label_size)\n",
    "        bar = plt.bar(xticks, result, align='center', width=0.9)\n",
    "\n",
    "        # avoid the label size too large\n",
    "        if label_size > 20:\n",
    "            labels = [i[:3] for i in labels]\n",
    "            rotation = 'vertical'\n",
    "            fontsize = 8\n",
    "        else:\n",
    "            labels = tuple(labels)\n",
    "            rotation = 30\n",
    "            fontsize = 6\n",
    "        plt.xticks(xticks, labels, fontsize=fontsize, rotation=rotation)\n",
    "        auto_label(aix=ax, rects=bar)\n",
    "\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        save_path = join(self.mother_path, file_name + '.jpg')\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        return save_path\n",
    "\n",
    "    def __check_extension(self, path):\n",
    "        assert os.path.exists(path), 'Path %s is not exit !' % path\n",
    "        if path.endswith(os.sep):\n",
    "            path = path[:-1]\n",
    "        f = os.path.split(path)[1]\n",
    "        if re.match(r'([\\w]+\\.(?:' + self.default_imagefile_extension + '))',\n",
    "                    f):\n",
    "            logging.debug('Check: %s, %s', \"True\", path)\n",
    "            return True\n",
    "        else:\n",
    "            logging.debug('Check: %s, %s', \"False\", path)\n",
    "            return False\n",
    "\n",
    "    def run(self, pred_path, predict_type, pred_filename=None, ext=None,\n",
    "            confusion_matrix=True, save_predict_image=True,\n",
    "            pandas_format=False):\n",
    "\n",
    "        if ext is not None and ext != '':\n",
    "            self.default_imagefile_extension = ext\n",
    "\n",
    "        assert os.path.exists(pred_path), \"Predict Path {} is not exist\"%(predict_path)\n",
    "\n",
    "\n",
    "        # just one image\n",
    "        if predict_type == MyPredict.NORMAL:\n",
    "            image_arrays, predict_list = self.__generate_array(pred_path)\n",
    "            results = self.model.predict(image_arrays)\n",
    "\n",
    "            # without label use [0,.....n] instead\n",
    "            self.label = self.labels if self.labels != None else [str(index) for\n",
    "                                                                  index in\n",
    "                                                                  np.arange(0,\n",
    "                                                                            len(\n",
    "                                                                                results[\n",
    "                                                                                    0]))]\n",
    "\n",
    "            df = None\n",
    "            if pandas_format:\n",
    "                columns = self.label + ['index']\n",
    "                df = pd.DataFrame(columns=columns)\n",
    "            for index, result in enumerate(results):\n",
    "                np.set_printoptions(linewidth=500, suppress=True)\n",
    "                print('MESSAGE:%s, \\t\\n%s' % (predict_list[index], result))\n",
    "\n",
    "                if pandas_format:\n",
    "                    result_list = list(result.copy())\n",
    "                    result_list.append(str(predict_list[index]))\n",
    "                    df = df.append({key: value for key, value in\n",
    "                                    zip(df.columns.values, result_list)},\n",
    "                                   ignore_index=True)\n",
    "\n",
    "                if save_predict_image:\n",
    "                    self.__save_image(image_arrays[0], result, pred_path,\n",
    "                                      MyPredict.generate_name())\n",
    "\n",
    "\n",
    "\n",
    "            if pandas_format:\n",
    "                return df\n",
    "\n",
    "        # one folder has many folder each folder's name is label\n",
    "        elif predict_type == MyPredict.CATEGORY:\n",
    "            labels, dir_paths = self.dir_paths(pred_path)\n",
    "            self.labels = labels\n",
    "            logging.debug('labels: %s, \\ndir_paths: %s', labels, dir_paths)\n",
    "\n",
    "            # log\n",
    "            if pred_filename is None or pred_filename == '':\n",
    "                predict_logfile_name = self.model.name + self.default_pred_filename_ext\n",
    "            else:\n",
    "                assert pred_filename.endswith('.csv')\n",
    "                predict_logfile_name = pred_filename\n",
    "\n",
    "            self.__init_csv()\n",
    "\n",
    "            for i, path in enumerate(dir_paths):\n",
    "                logging.info('Program read next directory: %s', path)\n",
    "                # read every directory's image file and generate image array\n",
    "                images, predict_list = self.__generate_array(path)\n",
    "                self.__predict(images, predict_list, i, save_image=save_predict_image)\n",
    "\n",
    "            # csv save\n",
    "            self.csv.to_csv(join(self.mother_path, predict_logfile_name),\n",
    "                            index=False)\n",
    "\n",
    "\n",
    "            logging.info('Logging Saved')\n",
    "\n",
    "            if confusion_matrix:\n",
    "                self.__generate_confusion_matrix()\n",
    "\n",
    "            if pandas_format:\n",
    "                return self.csv\n",
    "\n",
    "        # TODO 预测准确度\n",
    "        else:\n",
    "            raise RuntimeError(\"Predict type must be NORMAL or CATEGORY\")\n",
    "\n",
    "    def __generate_confusion_matrix(self):\n",
    "        matrix = np.zeros((len(self.labels),) * 2)\n",
    "        column_name = ['r_index', 't_index']\n",
    "        search_csv = self.csv[column_name]\n",
    "        np.set_printoptions(linewidth=500)\n",
    "        for index, row in search_csv.iterrows():\n",
    "            i, j = row[column_name[0]], row[column_name[1]]\n",
    "            matrix[int(i)][int(j)] += 1\n",
    "\n",
    "        confus_matrix = pd.DataFrame(matrix, index=self.labels,\n",
    "                                     columns=self.labels, dtype='int64')\n",
    "        pd.set_option('display.width', 500)\n",
    "        logging.info(\"MESSAGE: \\n%s\" % (confus_matrix))\n",
    "\n",
    "        confus_matrix.to_csv(self.default_confusion_matrix_path)\n",
    "        logging.info('Confusion Matrix Saved')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model_path = './weight/cifa.json'\n",
    "    weight_path = './weight/cifa.h5'\n",
    "    predict_path = './cifa'\n",
    "    # predict_path = '/Users/sober/Workspace/Python/PredictTools/Image/0491.jpg'\n",
    "    model = MyPredict.read_file_model(model_path, weight_path,\n",
    "                                      show_summary=True)\n",
    "#     model = MyPredict.read_log_model(name='xx', history_path='history')\n",
    "    predict = MyPredict(model=model, level=[logging.INFO], mother_path='xx')\n",
    "    predict.run(pred_filename='time.csv', predict_type=MyPredict.CATEGORY,\n",
    "                pred_path=predict_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from matplotlib.pyplot import imshow\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''\n",
    "\n",
    "def reformat(input):\n",
    "    output = []\n",
    "    print(type(input))\n",
    "    for i in input:\n",
    "       output.append('%.3f' %i)\n",
    "    return output\n",
    "\n",
    "model = load_model('weight/flower_gpu.h5')\n",
    "filepath = '/home/sober/ftp_bk/Dataset/Tmp_Train/flo/'\n",
    "for d in os.listdir(filepath):\n",
    "    dirpath = filepath + d + '/'\n",
    "    for f in os.listdir(dirpath):\n",
    "        file = dirpath + f\n",
    "        print(file)\n",
    "        im = Image.open(file)\n",
    "        im = im.resize((150, 150), Image.ANTIALIAS)\n",
    "        im = np.asarray(im)\n",
    "        im = im[np.newaxis, :]\n",
    "#         imshow(np.asarray(im))\n",
    "#         break\n",
    "        print(np.array2string(model.predict(np.asarray(im)), formatter={'float_kind':lambda x: \"%.2f\" % x}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFLite file Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import os\n",
    "filepath='./weight/lar_mobilenet.lite'\n",
    "# filepath='./weight/flower5.lite'\n",
    "val_path = '/home/sober/ftp_bk/Dataset/thesis_data/Val/'\n",
    "shape=(224, 224)\n",
    "max_predict = 10\n",
    "def pre_processing(image):\n",
    "    '''\n",
    "        image pre_processing\n",
    "    '''\n",
    "    return image / 255.0\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.contrib.lite.Interpreter(model_path=filepath)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "# change the following line to feed into your own data.\n",
    "\n",
    "\n",
    "for d in os.listdir(val_path):\n",
    "    dirpath = val_path+ d + '/'\n",
    "    for f in os.listdir(dirpath):\n",
    "        if max_predict < 0:\n",
    "            exit(0)\n",
    "        else:\n",
    "            max_predict -= 1\n",
    "        \n",
    "        file = dirpath + f\n",
    "        print(file)\n",
    "        im = Image.open(file)\n",
    "        im = im.resize(shape, Image.ANTIALIAS)\n",
    "        im = np.asarray(im)\n",
    "        im = pre_processing(im)\n",
    "        im = im[np.newaxis, :]\n",
    "        im = np.asarray(im)\n",
    "#         print(np.array2string(model.predict(np.asarray(im)), formatter={'float_kind':lambda x: \"%.2f\" % x}))\n",
    "        input_data = np.array(im, dtype=np.float32)\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "#         output_data.sort()\n",
    "        print(np.array2string(output_data, formatter={'float_kind':lambda x: \"%.2f\" % x}))\n",
    "    \n",
    "\"daisy dandelion roses sunflowers tulips\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow .pb, .tflite, get input and output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.platform import gfile\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "import os\n",
    "\n",
    "def save_graph_to_file(sess,  graph_file_name, output_names):\n",
    "    output_graph_def = graph_util.convert_variables_to_constants(\n",
    "      sess,  sess.graph.as_graph_def(),  output_names)\n",
    "    with gfile.FastGFile(graph_file_name, 'wb') as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "def keras2tflite(sess, input_tensors, output_tensors, filename):\n",
    "    '''\n",
    "    input_tensor and output_tensors is array and is keras input and output tensor\n",
    "    filename must be .tflite\n",
    "    '''\n",
    "    converter = tf.contrib.lite.TocoConverter.from_session(sess, input_tensors, output_tensors)\n",
    "    tflite_model=converter.convert()\n",
    "    open(filename, \"wb\").write(tflite_model)\n",
    "    \n",
    "def get_input_output_layer(filename) :\n",
    "    \"\"\"\n",
    "    It must clean the gpu memory, otherwise the input and output name is incorrect !\n",
    "    \"\"\"\n",
    "    if 'pb' in filename:\n",
    "        gf = tf.GraphDef()\n",
    "        gf.ParseFromString(open(filename,'rb').read())\n",
    "        for n in gf.node:\n",
    "            print(n.name + ' =>> ' + n.op )\n",
    "    elif 'h5' in filename:\n",
    "        assert(os.path.exists(filename))\n",
    "        model = load_model(filename)\n",
    "        input_names = [node.op.name for node in model.inputs]\n",
    "        output_names = [node.op.name for node in model.outputs]\n",
    "        print('Input :'  + str(input_names) + '\\nOutput: ' +  str(output_names))\n",
    "        \n",
    "get_input_output_layer('./weight/cifa.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Self-build Model seems Cat/Dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import os\n",
    "from keras import optimizers\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "train_path='/home/sober/code/jupyter/ftp_code/AndroidTensorflow/tensorflow-for-poets-2/tf_files/flower_photos'\n",
    "val_path='/home/sober/code/jupyter/ftp_code/AndroidTensorflow/tensorflow-for-poets-2/tf_files/flower_photos'\n",
    "batch_size = 16\n",
    "epochs = 200\n",
    "\n",
    "input = Input(shape = (150, 150, 3), name = 'input')\n",
    "x = Conv2D(32, (3, 3), activation='relu')(input)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "# x = Dense(64, activation='relu')(x)\n",
    "# x = Dense(2, activation='sigmoid', name = 'output')(x)\n",
    "x = Dense(5, activation='softmax', name = 'output')(x)\n",
    "model = Model(inputs = input, outputs= x)\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.000005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_path,  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        val_path,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=1340 // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        shuffle=True,\n",
    "        validation_steps=64 // batch_size)\n",
    "model.save('./weight/flower5.h5')  # always save your weights after training or during training\n",
    "\n",
    "# convert\n",
    "output_names = [node.op.name for node in model.outputs]\n",
    "\n",
    "export_dir = './weight/'\n",
    "sess = K.get_session()\n",
    "save_graph_to_file(sess,  export_dir + \"flower5.pb\", output_names)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.6.2",
   "language": "python",
   "name": "3.6.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
