{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python -V\n",
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                           Version           \n",
      "--------------------------------- ------------------\n",
      "absl-py                           0.2.2             \n",
      "astor                             0.6.2             \n",
      "autopep8                          1.4               \n",
      "backcall                          0.1.0             \n",
      "bleach                            1.5.0             \n",
      "certifi                           2018.8.24         \n",
      "chardet                           3.0.4             \n",
      "cloudpickle                       0.5.3             \n",
      "cycler                            0.10.0            \n",
      "Cython                            0.28.3            \n",
      "dask                              0.18.1            \n",
      "decorator                         4.3.0             \n",
      "defusedxml                        0.5.0             \n",
      "entrypoints                       0.2.3             \n",
      "gast                              0.2.0             \n",
      "graphviz                          0.8.4             \n",
      "grpcio                            1.13.0            \n",
      "h5py                              2.8.0             \n",
      "html5lib                          0.9999999         \n",
      "idna                              2.6               \n",
      "ipykernel                         5.1.0             \n",
      "ipyparallel                       6.2.2             \n",
      "ipython                           6.4.0             \n",
      "ipython-genutils                  0.2.0             \n",
      "ipywidgets                        7.4.2             \n",
      "jedi                              0.12.1            \n",
      "Jinja2                            2.10              \n",
      "jsonschema                        2.6.0             \n",
      "jupyter                           1.0.0             \n",
      "jupyter-client                    5.2.3             \n",
      "jupyter-console                   6.0.0             \n",
      "jupyter-contrib-core              0.3.3             \n",
      "jupyter-core                      4.4.0             \n",
      "jupyter-nbextensions-configurator 0.4.0             \n",
      "Keras                             2.2.4             \n",
      "Keras-Applications                1.0.6             \n",
      "Keras-Preprocessing               1.0.5             \n",
      "kiwisolver                        1.0.1             \n",
      "leveldb                           0.194             \n",
      "Markdown                          2.6.11            \n",
      "MarkupSafe                        1.0               \n",
      "matplotlib                        2.1.2             \n",
      "mistune                           0.8.4             \n",
      "mxnet                             1.3.0.post0       \n",
      "nbconvert                         5.4.0             \n",
      "nbformat                          4.4.0             \n",
      "networkx                          2.1               \n",
      "nose                              1.3.7             \n",
      "notebook                          5.7.0             \n",
      "numpy                             1.14.5            \n",
      "pandas                            0.23.3            \n",
      "pandocfilters                     1.4.2             \n",
      "parso                             0.3.0             \n",
      "pexpect                           4.6.0             \n",
      "pickleshare                       0.7.4             \n",
      "Pillow                            5.2.0             \n",
      "pip                               18.1              \n",
      "prometheus-client                 0.4.1             \n",
      "prompt-toolkit                    1.0.15            \n",
      "protobuf                          3.6.1             \n",
      "ptyprocess                        0.6.0             \n",
      "pycodestyle                       2.4.0             \n",
      "pydot                             1.2.4             \n",
      "Pygments                          2.2.0             \n",
      "pyparsing                         2.2.0             \n",
      "python-dateutil                   2.7.3             \n",
      "python-gflags                     3.1.2             \n",
      "pytz                              2018.5            \n",
      "PyWavelets                        0.5.2             \n",
      "PyYAML                            3.13              \n",
      "pyzmq                             17.1.2            \n",
      "qtconsole                         4.4.1             \n",
      "requests                          2.18.4            \n",
      "scikit-image                      0.14.0            \n",
      "scipy                             1.1.0             \n",
      "Send2Trash                        1.5.0             \n",
      "setuptools                        39.1.0            \n",
      "simplegeneric                     0.8.1             \n",
      "six                               1.11.0            \n",
      "tb-nightly                        1.12.0a20181013   \n",
      "tensorboard                       1.11.0            \n",
      "tensorflow-gpu                    1.11.0            \n",
      "termcolor                         1.1.0             \n",
      "terminado                         0.8.1             \n",
      "testpath                          0.4.2             \n",
      "tf-nightly                        1.12.0.dev20181012\n",
      "toolz                             0.9.0             \n",
      "tornado                           5.1.1             \n",
      "traitlets                         4.3.2             \n",
      "urllib3                           1.22              \n",
      "wcwidth                           0.1.7             \n",
      "Werkzeug                          0.14.1            \n",
      "wheel                             0.31.1            \n",
      "widgetsnbextension                3.4.2             \n",
      "yapf                              0.24.0            \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "FILE=flower_gpu\n",
    "\n",
    "tflite_convert \\\n",
    "  --output_file='./weight/mnist.tflite' \\\n",
    "  --keras_model_file='./weight/mnist.h5'\n",
    "  \n",
    "python keras_to_tensorflow.py --input_model=weight/${FILE}.h5 --output_model=weight/${FILE}.pb\n",
    "toco --graph_def_file='./weight/mnist.pb' --output_file='./weight/mnist.tflite' --input_shape='1,28,28,1' --input_arrays='input' --output_arrays='output/Softmax' --output_format=TFLITE  --input_format=TENSORFLOW_GRAPHDEF \n",
    "toco --graph_def_file='./weight/mnist.pb' --output_file='./weight/mnist.tflite' --input_shape='1,28,28,1' --input_arrays='input' --output_arrays='output/Softmax' --output_format=TFLITE  --input_format=TENSORFLOW_GRAPHDEF  -inference_type=FlOAT\n",
    "IMAGE_SIZE=224\n",
    "toco \\\n",
    "  --graph_def_file=weight/cifa.pb \\\n",
    "  --output_file=weight/cifa.lite \\\n",
    "  --input_format=TENSORFLOW_GRAPHDEF \\\n",
    "  --output_format=TFLITE \\\n",
    "  --input_shape=1,${IMAGE_SIZE},${IMAGE_SIZE},3 \\\n",
    "  --input_array=input_1 \\\n",
    "  --output_array=fc1000/Softmax \\\n",
    "  --inference_type=FLOAT \\\n",
    "  --input_data_type=FLOAT\n",
    "\n",
    "IMAGE_SIZE=224\n",
    "FILE=cifa\n",
    "INPUT=input_1\n",
    "OUTPUT=fc1000/Softmax\n",
    "toco \\\n",
    "  --graph_def_file=weight/${FILE}.pb \\\n",
    "  --output_file=weight/${FILE}.lite \\\n",
    "  --input_format=TENSORFLOW_GRAPHDEF \\\n",
    "  --output_format=TFLITE \\\n",
    "  --input_shape=1,${IMAGE_SIZE},${IMAGE_SIZE},3 \\\n",
    "  --input_array=${INPUT} \\\n",
    "  --output_array=${OUTPUT} \\\n",
    "  --inference_type=FLOAT \\\n",
    "  --input_data_type=FLOAT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    " \n",
    "mnist = input_data.read_data_sets(\"mnist\",one_hot=True)\n",
    " \n",
    " \n",
    "# 定义批次大小\n",
    "batch_size = 128\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "print(n_batch)\n",
    " \n",
    "# 定义placeholder\n",
    "x = tf.placeholder(tf.float32,[1,784],name='input_x')\n",
    "y = tf.placeholder(tf.float32,[1,10],name='output_y')\n",
    " \n",
    "# 定义 测试\n",
    "x_test = tf.placeholder(tf.float32,[None,784],name='input_test_x')\n",
    "y_test = tf.placeholder(tf.float32,[None,10],name='input_test_y')\n",
    " \n",
    "# 创建一个简单的神经网络\n",
    "W = tf.Variable(tf.zeros([784,10]),name=\"W\")\n",
    "b = tf.Variable(tf.zeros([1,10]),name=\"b\")\n",
    " \n",
    "prediction = tf.nn.softmax(tf.matmul(x,W)+b)\n",
    " \n",
    " \n",
    " \n",
    "# 创建损失函数\n",
    "train = tf.train.GradientDescentOptimizer(0.02).minimize(tf.reduce_mean(tf.square(y-prediction)))\n",
    " \n",
    "# 名称转换\n",
    "def canonical_name(x):\n",
    "  return x.name.split(\":\")[0]\n",
    " \n",
    "# 计算准确率\n",
    "test_prediction = tf.nn.softmax(tf.matmul(x_test,W)+b)\n",
    "accuarcy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_test,1),tf.argmax(test_prediction,1)),tf.float32))\n",
    " \n",
    "init = tf.global_variables_initializer()\n",
    "out = tf.identity(prediction, name=\"output\")\n",
    " \n",
    "print('Session')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(1):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "            for index in range(len(batch_xs)):\n",
    "                xs = batch_xs[index].reshape(1,784)\n",
    "                ys = batch_ys[index].reshape(1,10)\n",
    "                sess.run(train, feed_dict={x: xs, y: ys})\n",
    "            acc = sess.run(accuarcy,feed_dict={x_test:mnist.test.images,y_test:mnist.test.labels})\n",
    "            print(\"over\"+str(acc))\n",
    " \n",
    "    frozen_tensors = [out]\n",
    "    out_tensors = [out]\n",
    " \n",
    "    frozen_graphdef = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, list(map(canonical_name, frozen_tensors)))\n",
    "    tflite_model = tf.contrib.lite.toco_convert(frozen_graphdef, [x], out_tensors)\n",
    " \n",
    "    open(\"writer_model.tflite\", \"wb\").write(tflite_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import plot_model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "(60000,) 5\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.2781 - acc: 0.9138 - val_loss: 0.0628 - val_acc: 0.9797\n",
      "Epoch 2/5\n",
      "24320/60000 [===========>..................] - ETA: 2s - loss: 0.0979 - acc: 0.9724"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e594631d3de3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./weight/mnist.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "'''\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print(y_train.shape, y_train[0])\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "input = Input(name ='input', shape = input_shape)\n",
    "x = Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu')(input)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(num_classes, activation='softmax', name = 'output')(x)\n",
    "model = Model(inputs = input, outputs = x)\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "model.summary()\n",
    "model.save('./weight/mnist.h5')\n",
    "with open('./weight/mnist.json', 'w+') as f:\n",
    "    f.write(model.to_json())\n",
    "plot_model(model, to_file='./weight/mnist.png', show_shapes = True)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "os.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifar Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 images belonging to 10 classes.\n",
      "Found 10000 images belonging to 10 classes.\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sober/.pyenv/versions/3.6.2/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 65s 103ms/step - loss: 1.9396 - acc: 0.3548 - val_loss: 4.1216 - val_acc: 0.1650\n",
      "Epoch 2/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 1.6061 - acc: 0.5388 - val_loss: 2.5510 - val_acc: 0.1650\n",
      "Epoch 3/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 1.2895 - acc: 0.6522 - val_loss: 2.6177 - val_acc: 0.1900\n",
      "Epoch 4/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.9751 - acc: 0.7352 - val_loss: 2.3249 - val_acc: 0.1650\n",
      "Epoch 5/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.7859 - acc: 0.7840 - val_loss: 2.1421 - val_acc: 0.2300\n",
      "Epoch 6/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.7014 - acc: 0.7998 - val_loss: 2.8698 - val_acc: 0.1950\n",
      "Epoch 7/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.6359 - acc: 0.8214 - val_loss: 2.2235 - val_acc: 0.2700\n",
      "Epoch 8/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.5987 - acc: 0.8406 - val_loss: 2.4147 - val_acc: 0.2000\n",
      "Epoch 9/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.5923 - acc: 0.8442 - val_loss: 1.9777 - val_acc: 0.3100\n",
      "Epoch 10/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.5597 - acc: 0.8448 - val_loss: 2.0924 - val_acc: 0.2900\n",
      "Epoch 11/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.5594 - acc: 0.8440 - val_loss: 2.3840 - val_acc: 0.2250\n",
      "Epoch 12/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.5163 - acc: 0.8490 - val_loss: 2.5587 - val_acc: 0.2100\n",
      "Epoch 13/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4871 - acc: 0.8506 - val_loss: 2.5987 - val_acc: 0.1800\n",
      "Epoch 14/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4812 - acc: 0.8520 - val_loss: 2.4200 - val_acc: 0.2850\n",
      "Epoch 15/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4786 - acc: 0.8524 - val_loss: 2.3417 - val_acc: 0.2250\n",
      "Epoch 16/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4664 - acc: 0.8532 - val_loss: 2.3043 - val_acc: 0.2350\n",
      "Epoch 17/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4751 - acc: 0.8516 - val_loss: 2.3208 - val_acc: 0.2300\n",
      "Epoch 18/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4850 - acc: 0.8508 - val_loss: 2.2384 - val_acc: 0.2700\n",
      "Epoch 19/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4843 - acc: 0.8476 - val_loss: 1.9566 - val_acc: 0.3400\n",
      "Epoch 20/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4700 - acc: 0.8524 - val_loss: 2.4255 - val_acc: 0.2200\n",
      "Epoch 21/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4674 - acc: 0.8530 - val_loss: 2.6750 - val_acc: 0.2150\n",
      "Epoch 22/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4528 - acc: 0.8592 - val_loss: 2.6596 - val_acc: 0.1850\n",
      "Epoch 23/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4464 - acc: 0.8562 - val_loss: 2.0179 - val_acc: 0.3300\n",
      "Epoch 24/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4677 - acc: 0.8490 - val_loss: 2.1839 - val_acc: 0.2350\n",
      "Epoch 25/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4499 - acc: 0.8560 - val_loss: 2.0956 - val_acc: 0.2450\n",
      "Epoch 26/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4486 - acc: 0.8576 - val_loss: 1.9615 - val_acc: 0.3450\n",
      "Epoch 27/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4448 - acc: 0.8574 - val_loss: 2.0357 - val_acc: 0.3150\n",
      "Epoch 28/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4576 - acc: 0.8534 - val_loss: 2.3877 - val_acc: 0.2550\n",
      "Epoch 29/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4458 - acc: 0.8594 - val_loss: 2.4421 - val_acc: 0.2150\n",
      "Epoch 30/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4440 - acc: 0.8600 - val_loss: 2.3747 - val_acc: 0.2550\n",
      "Epoch 31/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4614 - acc: 0.8524 - val_loss: 2.2844 - val_acc: 0.1900\n",
      "Epoch 32/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4472 - acc: 0.8552 - val_loss: 2.2090 - val_acc: 0.2350\n",
      "Epoch 33/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4411 - acc: 0.8598 - val_loss: 2.2785 - val_acc: 0.2800\n",
      "Epoch 34/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4445 - acc: 0.8550 - val_loss: 2.3012 - val_acc: 0.3000\n",
      "Epoch 35/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4502 - acc: 0.8572 - val_loss: 2.5141 - val_acc: 0.2300\n",
      "Epoch 36/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4441 - acc: 0.8562 - val_loss: 2.2095 - val_acc: 0.2850\n",
      "Epoch 37/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4654 - acc: 0.8504 - val_loss: 2.2963 - val_acc: 0.2700\n",
      "Epoch 38/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4368 - acc: 0.8566 - val_loss: 2.2446 - val_acc: 0.2900\n",
      "Epoch 39/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4496 - acc: 0.8556 - val_loss: 2.5971 - val_acc: 0.3150\n",
      "Epoch 40/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4522 - acc: 0.8558 - val_loss: 2.0308 - val_acc: 0.3300\n",
      "Epoch 41/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4493 - acc: 0.8598 - val_loss: 2.3196 - val_acc: 0.3050\n",
      "Epoch 42/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4565 - acc: 0.8576 - val_loss: 2.2488 - val_acc: 0.2800\n",
      "Epoch 43/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4617 - acc: 0.8558 - val_loss: 2.0470 - val_acc: 0.3450\n",
      "Epoch 44/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4487 - acc: 0.8582 - val_loss: 2.4118 - val_acc: 0.2400\n",
      "Epoch 45/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4341 - acc: 0.8614 - val_loss: 2.0426 - val_acc: 0.3200\n",
      "Epoch 46/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4523 - acc: 0.8560 - val_loss: 2.3116 - val_acc: 0.2400\n",
      "Epoch 47/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4408 - acc: 0.8598 - val_loss: 2.0150 - val_acc: 0.3100\n",
      "Epoch 48/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4183 - acc: 0.8660 - val_loss: 2.3489 - val_acc: 0.2600\n",
      "Epoch 49/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4549 - acc: 0.8540 - val_loss: 2.0026 - val_acc: 0.3300\n",
      "Epoch 50/200\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.4487 - acc: 0.8614 - val_loss: 2.0824 - val_acc: 0.2750\n",
      "Epoch 51/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4488 - acc: 0.8568 - val_loss: 2.3476 - val_acc: 0.2800\n",
      "Epoch 52/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4203 - acc: 0.8678 - val_loss: 2.3496 - val_acc: 0.2700\n",
      "Epoch 53/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4408 - acc: 0.8588 - val_loss: 2.1141 - val_acc: 0.3500\n",
      "Epoch 54/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4397 - acc: 0.8630 - val_loss: 2.0405 - val_acc: 0.3450\n",
      "Epoch 55/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4377 - acc: 0.8600 - val_loss: 2.2395 - val_acc: 0.2800\n",
      "Epoch 56/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4269 - acc: 0.8684 - val_loss: 2.4544 - val_acc: 0.2500\n",
      "Epoch 57/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4380 - acc: 0.8594 - val_loss: 1.8835 - val_acc: 0.3900\n",
      "Epoch 58/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4465 - acc: 0.8582 - val_loss: 2.0157 - val_acc: 0.3300\n",
      "Epoch 59/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4416 - acc: 0.8608 - val_loss: 2.0484 - val_acc: 0.3450\n",
      "Epoch 60/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4382 - acc: 0.8630 - val_loss: 2.2111 - val_acc: 0.2900\n",
      "Epoch 61/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4284 - acc: 0.8644 - val_loss: 1.8504 - val_acc: 0.3000\n",
      "Epoch 62/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4540 - acc: 0.8600 - val_loss: 2.0268 - val_acc: 0.3300\n",
      "Epoch 63/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4408 - acc: 0.8590 - val_loss: 2.3691 - val_acc: 0.3200\n",
      "Epoch 64/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4335 - acc: 0.8658 - val_loss: 2.1272 - val_acc: 0.2600\n",
      "Epoch 65/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4330 - acc: 0.8620 - val_loss: 1.9868 - val_acc: 0.3100\n",
      "Epoch 66/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4281 - acc: 0.8646 - val_loss: 1.8293 - val_acc: 0.3450\n",
      "Epoch 67/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4120 - acc: 0.8686 - val_loss: 2.1752 - val_acc: 0.2900\n",
      "Epoch 68/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4318 - acc: 0.8624 - val_loss: 2.3019 - val_acc: 0.2550\n",
      "Epoch 69/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4224 - acc: 0.8594 - val_loss: 2.0142 - val_acc: 0.3100\n",
      "Epoch 70/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4488 - acc: 0.8590 - val_loss: 2.2174 - val_acc: 0.2800\n",
      "Epoch 71/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4359 - acc: 0.8594 - val_loss: 2.0598 - val_acc: 0.3350\n",
      "Epoch 72/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4320 - acc: 0.8646 - val_loss: 2.1659 - val_acc: 0.3200\n",
      "Epoch 73/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4498 - acc: 0.8574 - val_loss: 2.4454 - val_acc: 0.2650\n",
      "Epoch 74/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4327 - acc: 0.8628 - val_loss: 2.1188 - val_acc: 0.2650\n",
      "Epoch 75/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4335 - acc: 0.8652 - val_loss: 2.0054 - val_acc: 0.3400\n",
      "Epoch 76/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4555 - acc: 0.8580 - val_loss: 1.8969 - val_acc: 0.3350\n",
      "Epoch 77/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4246 - acc: 0.8632 - val_loss: 2.0504 - val_acc: 0.3350\n",
      "Epoch 78/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4285 - acc: 0.8620 - val_loss: 2.3372 - val_acc: 0.3300\n",
      "Epoch 79/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4269 - acc: 0.8626 - val_loss: 2.0636 - val_acc: 0.3350\n",
      "Epoch 80/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4386 - acc: 0.8566 - val_loss: 1.9963 - val_acc: 0.4000\n",
      "Epoch 81/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4153 - acc: 0.8632 - val_loss: 2.1082 - val_acc: 0.3500\n",
      "Epoch 82/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4472 - acc: 0.8616 - val_loss: 2.1726 - val_acc: 0.3400\n",
      "Epoch 83/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4110 - acc: 0.8732 - val_loss: 1.8246 - val_acc: 0.3650\n",
      "Epoch 84/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4207 - acc: 0.8648 - val_loss: 1.9288 - val_acc: 0.3700\n",
      "Epoch 85/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4299 - acc: 0.8632 - val_loss: 1.9345 - val_acc: 0.3000\n",
      "Epoch 86/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4198 - acc: 0.8656 - val_loss: 1.9599 - val_acc: 0.3850\n",
      "Epoch 87/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4143 - acc: 0.8636 - val_loss: 2.1409 - val_acc: 0.3450\n",
      "Epoch 88/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4342 - acc: 0.8670 - val_loss: 2.0972 - val_acc: 0.3400\n",
      "Epoch 89/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4166 - acc: 0.8664 - val_loss: 2.0214 - val_acc: 0.3550\n",
      "Epoch 90/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4189 - acc: 0.8636 - val_loss: 2.0544 - val_acc: 0.3150\n",
      "Epoch 91/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4185 - acc: 0.8650 - val_loss: 1.9891 - val_acc: 0.3550\n",
      "Epoch 92/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4111 - acc: 0.8686 - val_loss: 1.9844 - val_acc: 0.3250\n",
      "Epoch 93/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4112 - acc: 0.8682 - val_loss: 1.9085 - val_acc: 0.3350\n",
      "Epoch 94/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4291 - acc: 0.8644 - val_loss: 2.0846 - val_acc: 0.3400\n",
      "Epoch 95/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4202 - acc: 0.8640 - val_loss: 2.0369 - val_acc: 0.3050\n",
      "Epoch 96/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4340 - acc: 0.8592 - val_loss: 2.2593 - val_acc: 0.3700\n",
      "Epoch 97/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4092 - acc: 0.8642 - val_loss: 2.0059 - val_acc: 0.3500\n",
      "Epoch 98/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4171 - acc: 0.8670 - val_loss: 2.1704 - val_acc: 0.3550\n",
      "Epoch 99/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4215 - acc: 0.8682 - val_loss: 1.8127 - val_acc: 0.4250\n",
      "Epoch 100/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4252 - acc: 0.8662 - val_loss: 2.0269 - val_acc: 0.3650\n",
      "Epoch 101/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3998 - acc: 0.8728 - val_loss: 2.0157 - val_acc: 0.3800\n",
      "Epoch 102/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4173 - acc: 0.8656 - val_loss: 2.1366 - val_acc: 0.3100\n",
      "Epoch 103/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4134 - acc: 0.8696 - val_loss: 2.0759 - val_acc: 0.3000\n",
      "Epoch 104/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4072 - acc: 0.8650 - val_loss: 1.8385 - val_acc: 0.3700\n",
      "Epoch 105/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4201 - acc: 0.8658 - val_loss: 1.7640 - val_acc: 0.4250\n",
      "Epoch 106/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4163 - acc: 0.8684 - val_loss: 1.9450 - val_acc: 0.3600\n",
      "Epoch 107/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4111 - acc: 0.8704 - val_loss: 1.6238 - val_acc: 0.3950\n",
      "Epoch 108/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4023 - acc: 0.8722 - val_loss: 2.1182 - val_acc: 0.3900\n",
      "Epoch 109/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4047 - acc: 0.8698 - val_loss: 1.9182 - val_acc: 0.3550\n",
      "Epoch 110/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4135 - acc: 0.8656 - val_loss: 2.2797 - val_acc: 0.2950\n",
      "Epoch 111/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4043 - acc: 0.8652 - val_loss: 2.0313 - val_acc: 0.3400\n",
      "Epoch 112/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3978 - acc: 0.8704 - val_loss: 1.8983 - val_acc: 0.3900\n",
      "Epoch 113/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3999 - acc: 0.8688 - val_loss: 2.1626 - val_acc: 0.3400\n",
      "Epoch 114/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4230 - acc: 0.8624 - val_loss: 2.1683 - val_acc: 0.3000\n",
      "Epoch 115/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4190 - acc: 0.8680 - val_loss: 1.7889 - val_acc: 0.4000\n",
      "Epoch 116/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4111 - acc: 0.8656 - val_loss: 2.1694 - val_acc: 0.2950\n",
      "Epoch 117/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4116 - acc: 0.8648 - val_loss: 1.9997 - val_acc: 0.3350\n",
      "Epoch 118/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4194 - acc: 0.8632 - val_loss: 1.8903 - val_acc: 0.3450\n",
      "Epoch 119/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4024 - acc: 0.8710 - val_loss: 1.7662 - val_acc: 0.4000\n",
      "Epoch 120/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4153 - acc: 0.8670 - val_loss: 1.8012 - val_acc: 0.4050\n",
      "Epoch 121/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3909 - acc: 0.8742 - val_loss: 1.8068 - val_acc: 0.4150\n",
      "Epoch 122/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4040 - acc: 0.8698 - val_loss: 2.0179 - val_acc: 0.3550\n",
      "Epoch 123/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4251 - acc: 0.8670 - val_loss: 1.8832 - val_acc: 0.3850\n",
      "Epoch 124/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3805 - acc: 0.8754 - val_loss: 1.5677 - val_acc: 0.5100\n",
      "Epoch 125/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4212 - acc: 0.8620 - val_loss: 1.9087 - val_acc: 0.3400\n",
      "Epoch 126/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4117 - acc: 0.8666 - val_loss: 1.8925 - val_acc: 0.3800\n",
      "Epoch 127/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4008 - acc: 0.8716 - val_loss: 2.0848 - val_acc: 0.3200\n",
      "Epoch 128/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4107 - acc: 0.8652 - val_loss: 1.7641 - val_acc: 0.4650\n",
      "Epoch 129/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3949 - acc: 0.8710 - val_loss: 1.7121 - val_acc: 0.3950\n",
      "Epoch 130/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3982 - acc: 0.8734 - val_loss: 1.9809 - val_acc: 0.3350\n",
      "Epoch 131/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4195 - acc: 0.8686 - val_loss: 1.8186 - val_acc: 0.4050\n",
      "Epoch 132/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4286 - acc: 0.8630 - val_loss: 1.6741 - val_acc: 0.4050\n",
      "Epoch 133/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3937 - acc: 0.8736 - val_loss: 2.1099 - val_acc: 0.3250\n",
      "Epoch 134/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4027 - acc: 0.8672 - val_loss: 1.7291 - val_acc: 0.3950\n",
      "Epoch 135/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4022 - acc: 0.8694 - val_loss: 1.8576 - val_acc: 0.3850\n",
      "Epoch 136/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3922 - acc: 0.8714 - val_loss: 1.6517 - val_acc: 0.4250\n",
      "Epoch 137/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3797 - acc: 0.8776 - val_loss: 1.7736 - val_acc: 0.3950\n",
      "Epoch 138/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4059 - acc: 0.8648 - val_loss: 1.9104 - val_acc: 0.4050\n",
      "Epoch 139/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4180 - acc: 0.8662 - val_loss: 1.7887 - val_acc: 0.4050\n",
      "Epoch 140/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4003 - acc: 0.8708 - val_loss: 1.9263 - val_acc: 0.3700\n",
      "Epoch 141/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3918 - acc: 0.8730 - val_loss: 1.8628 - val_acc: 0.4050\n",
      "Epoch 142/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3884 - acc: 0.8754 - val_loss: 1.8732 - val_acc: 0.3150\n",
      "Epoch 143/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3887 - acc: 0.8768 - val_loss: 1.7903 - val_acc: 0.3950\n",
      "Epoch 144/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3951 - acc: 0.8786 - val_loss: 1.7201 - val_acc: 0.3950\n",
      "Epoch 145/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4036 - acc: 0.8692 - val_loss: 1.8797 - val_acc: 0.3500\n",
      "Epoch 146/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4162 - acc: 0.8686 - val_loss: 1.8870 - val_acc: 0.3600\n",
      "Epoch 147/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4035 - acc: 0.8730 - val_loss: 1.8562 - val_acc: 0.3950\n",
      "Epoch 148/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3907 - acc: 0.8756 - val_loss: 2.0396 - val_acc: 0.3800\n",
      "Epoch 149/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4035 - acc: 0.8716 - val_loss: 1.8730 - val_acc: 0.3850\n",
      "Epoch 150/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3998 - acc: 0.8746 - val_loss: 2.0451 - val_acc: 0.3550\n",
      "Epoch 151/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3982 - acc: 0.8748 - val_loss: 1.6111 - val_acc: 0.4450\n",
      "Epoch 152/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3943 - acc: 0.8758 - val_loss: 1.7055 - val_acc: 0.3650\n",
      "Epoch 153/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3945 - acc: 0.8786 - val_loss: 2.1410 - val_acc: 0.3050\n",
      "Epoch 154/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3898 - acc: 0.8764 - val_loss: 1.6976 - val_acc: 0.4200\n",
      "Epoch 155/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3877 - acc: 0.8774 - val_loss: 1.9002 - val_acc: 0.4250\n",
      "Epoch 156/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3864 - acc: 0.8786 - val_loss: 1.7103 - val_acc: 0.3850\n",
      "Epoch 157/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4062 - acc: 0.8764 - val_loss: 1.7413 - val_acc: 0.3750\n",
      "Epoch 158/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3859 - acc: 0.8718 - val_loss: 1.8132 - val_acc: 0.4050\n",
      "Epoch 159/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3670 - acc: 0.8814 - val_loss: 2.1550 - val_acc: 0.3050\n",
      "Epoch 160/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3991 - acc: 0.8722 - val_loss: 1.9885 - val_acc: 0.4300\n",
      "Epoch 161/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3895 - acc: 0.8742 - val_loss: 1.8142 - val_acc: 0.4250\n",
      "Epoch 162/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4052 - acc: 0.8736 - val_loss: 1.9810 - val_acc: 0.4100\n",
      "Epoch 163/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3888 - acc: 0.8796 - val_loss: 1.7333 - val_acc: 0.4800\n",
      "Epoch 164/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3956 - acc: 0.8740 - val_loss: 1.7775 - val_acc: 0.3750\n",
      "Epoch 165/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3951 - acc: 0.8722 - val_loss: 1.6163 - val_acc: 0.4650\n",
      "Epoch 166/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3934 - acc: 0.8730 - val_loss: 1.8083 - val_acc: 0.4150\n",
      "Epoch 167/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3850 - acc: 0.8768 - val_loss: 1.8154 - val_acc: 0.3950\n",
      "Epoch 168/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3880 - acc: 0.8772 - val_loss: 1.8112 - val_acc: 0.4050\n",
      "Epoch 169/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3861 - acc: 0.8802 - val_loss: 1.9978 - val_acc: 0.3750\n",
      "Epoch 170/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4081 - acc: 0.8740 - val_loss: 1.7458 - val_acc: 0.4150\n",
      "Epoch 171/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3984 - acc: 0.8714 - val_loss: 1.6424 - val_acc: 0.4300\n",
      "Epoch 172/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3880 - acc: 0.8768 - val_loss: 1.7482 - val_acc: 0.4250\n",
      "Epoch 173/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3890 - acc: 0.8806 - val_loss: 1.8543 - val_acc: 0.4150\n",
      "Epoch 174/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4039 - acc: 0.8790 - val_loss: 1.8091 - val_acc: 0.4400\n",
      "Epoch 175/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4017 - acc: 0.8730 - val_loss: 1.9063 - val_acc: 0.3800\n",
      "Epoch 176/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3887 - acc: 0.8746 - val_loss: 1.9655 - val_acc: 0.4200\n",
      "Epoch 177/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3929 - acc: 0.8762 - val_loss: 1.6344 - val_acc: 0.4650\n",
      "Epoch 178/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3933 - acc: 0.8758 - val_loss: 1.6782 - val_acc: 0.4600\n",
      "Epoch 179/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3988 - acc: 0.8764 - val_loss: 1.7962 - val_acc: 0.4200\n",
      "Epoch 180/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3944 - acc: 0.8778 - val_loss: 1.8260 - val_acc: 0.4050\n",
      "Epoch 181/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3725 - acc: 0.8816 - val_loss: 1.7041 - val_acc: 0.4600\n",
      "Epoch 182/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3795 - acc: 0.8770 - val_loss: 1.7179 - val_acc: 0.4550\n",
      "Epoch 183/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4118 - acc: 0.8746 - val_loss: 1.8242 - val_acc: 0.3800\n",
      "Epoch 184/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3925 - acc: 0.8736 - val_loss: 1.6490 - val_acc: 0.4550\n",
      "Epoch 185/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3701 - acc: 0.8842 - val_loss: 1.7216 - val_acc: 0.3900\n",
      "Epoch 186/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3933 - acc: 0.8792 - val_loss: 1.9202 - val_acc: 0.4450\n",
      "Epoch 187/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.4058 - acc: 0.8734 - val_loss: 1.6774 - val_acc: 0.4500\n",
      "Epoch 188/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3752 - acc: 0.8768 - val_loss: 1.6078 - val_acc: 0.4400\n",
      "Epoch 189/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3880 - acc: 0.8790 - val_loss: 1.6459 - val_acc: 0.4650\n",
      "Epoch 190/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3853 - acc: 0.8766 - val_loss: 1.8740 - val_acc: 0.3750\n",
      "Epoch 191/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3767 - acc: 0.8828 - val_loss: 1.5694 - val_acc: 0.4900\n",
      "Epoch 192/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3777 - acc: 0.8806 - val_loss: 1.7540 - val_acc: 0.4050\n",
      "Epoch 193/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3853 - acc: 0.8774 - val_loss: 1.6598 - val_acc: 0.4200\n",
      "Epoch 194/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3778 - acc: 0.8784 - val_loss: 1.5769 - val_acc: 0.4550\n",
      "Epoch 195/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3881 - acc: 0.8778 - val_loss: 1.7435 - val_acc: 0.4150\n",
      "Epoch 196/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3929 - acc: 0.8796 - val_loss: 2.0844 - val_acc: 0.3700\n",
      "Epoch 197/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3943 - acc: 0.8772 - val_loss: 1.7325 - val_acc: 0.4050\n",
      "Epoch 198/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3788 - acc: 0.8840 - val_loss: 1.7468 - val_acc: 0.4700\n",
      "Epoch 199/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3830 - acc: 0.8812 - val_loss: 1.9341 - val_acc: 0.3400\n",
      "Epoch 200/200\n",
      "625/625 [==============================] - 57s 91ms/step - loss: 0.3737 - acc: 0.8816 - val_loss: 1.8738 - val_acc: 0.3850\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-70b6ab3c9866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./weight/cifa.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./weight/cifa.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         raise ImportError(\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "img_width, img_height = 224, 224\n",
    "train_data_dir = '/home/sober/ftp_bk/Dataset/Tmp_Train/cifar/Train/'\n",
    "val_data_dir = '/home/sober/ftp_bk/Dataset/Tmp_Train/cifar/Val/'\n",
    "batch_size = 8\n",
    "epochs = 200\n",
    "nb_train_samples, nb_validation_samples = 5000,  1000\n",
    "\n",
    "\n",
    "def mean(x):\n",
    "    return (x-128.0)/128.0\n",
    "\n",
    "model = ResNet50(include_top = True, classes = 10, weights=None, input_shape=(img_width, img_height, 3))\n",
    "# model.summary()\n",
    "# model.layers.pop()\n",
    "# output = model.layers[-1].output\n",
    "# output = Flatten()(output)\n",
    "# output = Dense(10, activation='softmax')(output)\n",
    "# model = Model(model.input, output)\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    \n",
    "train_datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "    vertical_flip=True, rotation_range=15, preprocessing_function=mean)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=mean)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "            train_data_dir, shuffle=True,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=batch_size, class_mode='categorical')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "            val_data_dir, shuffle=True,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           verbose=1,\n",
    "#           validation_data=(x_test, y_test))\n",
    "# EarlyStopping(monitor='val_acc', patience=5,\n",
    "#                                       verbose=0,\n",
    "#                                       mode='auto'),\n",
    "# ReduceLROnPlateau(monitor='val_acc', factor=0.2,\n",
    "#                                               patience=10, verbose=0,\n",
    "#                                               cooldown=1, epsilon=0.001)\n",
    "\n",
    "callbacks = []\n",
    "historys = model.fit_generator(callbacks=callbacks,\n",
    "                                                use_multiprocessing=True,\n",
    "                                                workers=5, max_queue_size=20,\n",
    "                                                generator=train_generator,\n",
    "                                                steps_per_epoch=nb_train_samples // batch_size,\n",
    "                                                epochs=epochs,\n",
    "                                                validation_steps=nb_validation_samples // batch_size,\n",
    "                                                validation_data=validation_generator)\n",
    "# model.summary()\n",
    "model.save('./weight/cifa.h5')\n",
    "with open('./weight/cifa.json', 'w+') as f:\n",
    "    f.write(model.to_json())\n",
    "plot_model(model, to_file='./weight/cifa.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifar Keras Model Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.models import model_from_json\n",
    "from os import listdir\n",
    "from LoadWriteModel import loadWriteModel\n",
    "from os.path import join, isdir\n",
    "import gc\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "\n",
    "class MyPredict(object):\n",
    "    # directory\n",
    "    CATEGORY = 20\n",
    "    # file\n",
    "    NORMAL = 10\n",
    "\n",
    "    def __init__(self, model, labels=None, image_shape=(224, 224), level=None,\n",
    "                 mother_path='./Predict'):\n",
    "        \"\"\"\n",
    "        :param model:\n",
    "        :param labels:\n",
    "        :param image_shape:\n",
    "        :param level:   [console level] or ['console level', 'log level']\n",
    "        logging.INFO\n",
    "        :param mother_path:     log and predict summary image save file path\n",
    "        \"\"\"\n",
    "\n",
    "        self.mother_path = mother_path\n",
    "        self.labels = labels\n",
    "        self.image_shape = image_shape\n",
    "        self.__init_default_variable()\n",
    "        self.__init_environment()\n",
    "        self.__init_log(\n",
    "            level=level if level is not None else self.default_level)\n",
    "        self.model = model\n",
    "        logging.info('Init finish')\n",
    "\n",
    "    def __init_default_variable(self):\n",
    "        \"\"\"\n",
    "        the default variable and path\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.default_logname = 'MyPredict_log.log'\n",
    "        self.default_level = [logging.DEBUG]\n",
    "        self.default_pred_filename_ext = '_predict.csv'\n",
    "        self.default_confusion_matrix_path = join(self.mother_path,\n",
    "                                                  'confusion_matrix.csv')\n",
    "        self.default_imagefile_extension = 'jpg|jpeg|bmp|png'\n",
    "\n",
    "    def __init_environment(self):\n",
    "        # log remove\n",
    "        for root, _, files in os.walk('./'):\n",
    "            for f in files:\n",
    "                if f.endswith(r'.log'):\n",
    "                    os.remove(join(root, f))\n",
    "\n",
    "        # image save path clean\n",
    "        if os.path.exists(self.mother_path):\n",
    "            shutil.rmtree(self.mother_path)\n",
    "\n",
    "        # create mother folder\n",
    "        os.makedirs(self.mother_path)\n",
    "\n",
    "    def __init_log(self, level):\n",
    "        \"\"\"\n",
    "        :param level:  level[0] is log file output level, level[1] is console output level\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if len(level) == 1:\n",
    "            if type(level) is list:\n",
    "                level = level[0]\n",
    "            logging.basicConfig(level=level)\n",
    "        elif len(level) == 2:\n",
    "            logging.basicConfig(filename=self.default_logname, level=level[1])\n",
    "            console = logging.StreamHandler()\n",
    "            console.setLevel(level[0])\n",
    "            logging.getLogger('').addHandler(console)\n",
    "        else:\n",
    "            raise RuntimeError(\"The log level's length must be 1 or 2\")\n",
    "        logging.info('Log init')\n",
    "\n",
    "    # def set_output_level(self, level):\n",
    "    #     size = len(level)\n",
    "    #     assert (size == 1 or size == 2),  'Level length must be 0 or 1'\n",
    "    #     if size == 1:\n",
    "    #         level = level[0] if type(level) is list\n",
    "    #         logging.Logger.setLevel(level)\n",
    "    #     else:\n",
    "    #         console.setLevel(level[1])\n",
    "    #         log\n",
    "\n",
    "    def set_labels(self, labels):\n",
    "        \"\"\"\n",
    "        :param labels:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.labels is None or self.labels == '':\n",
    "            logging.info(\"Label Change\\n Origin: %s\\n Now: %s\\n\", self.labels,\n",
    "                         labels)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __del__(self):\n",
    "        self.label = None\n",
    "        del self.model\n",
    "        self.model = None\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    def __init_csv(self):\n",
    "        \"\"\"\n",
    "        init the log file\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        columns_name = ['file', 'r_label', 'r_index', 't_index', 't_label',\n",
    "                        'timestamp', 'image', 'check']\n",
    "        for index in self.labels:\n",
    "            columns_name.append('l_' + index)\n",
    "        columns_name = sorted(columns_name)\n",
    "        self.csv = pd.DataFrame(columns=columns_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_log_model(name, history_path, history_load=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param name:\n",
    "        :param history_path:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        read from history file and generate model name if not given\n",
    "        :param name:    model name which need to be load\n",
    "        :param history: history path\n",
    "        :param model:   model\n",
    "        :return:    loaded model\n",
    "        \"\"\"\n",
    "        assert history_path != None and os.path.exists(\n",
    "            history_path), 'History path ' \\\n",
    "                           'must be valid'\n",
    "        assert name != None\n",
    "        loadboostrap = loadWriteModel(history_path)\n",
    "        model, _ = loadboostrap.load(name, weight_load=True,\n",
    "                                     history_load=history_load)\n",
    "        model.name = name\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def read_file_model(model_path, weight_path, show_summary=False):\n",
    "        \"\"\" read model and load weight from json and h5 file\n",
    "\n",
    "        model_path = './Result/model/i_shift.json'\n",
    "        weight_path = './Result/weight/i_shift.h5'\n",
    "        model = MyPredict.read_file_model(model_path, weight_path, show_summary=True)\n",
    "\n",
    "        :param model_path:\n",
    "        :param weight_path:\n",
    "        :param show_summary:    show model summary or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        assert (os.path.exists(model_path) is True and model_path.endswith(\n",
    "            '.json')), 'Model Path invalid'\n",
    "        assert (os.path.exists(weight_path) is True and weight_path.endswith(\n",
    "            '.h5')), 'Weight Path invalid'\n",
    "        with open(model_path, 'r') as f:\n",
    "            model = model_from_json(f.read())\n",
    "        model.load_weights(weight_path, by_name=True)\n",
    "        end = time.time()\n",
    "        print('Model Loaded Spend:%0.3fs' % (end - start))\n",
    "        if show_summary:\n",
    "            model.summary()\n",
    "\n",
    "        if model.name is None:\n",
    "            model.name = MyPredict.generate_name()\n",
    "        return model\n",
    "\n",
    "    # def __read_model(self, name=None, history=None, model=None):\n",
    "    #     \"\"\"\n",
    "    #     read from history file and generate model name if not given\n",
    "    #     :param name:    model name which need to be load\n",
    "    #     :param history: history path\n",
    "    #     :param model:   model\n",
    "    #     :return:    loaded model\n",
    "    #     \"\"\"\n",
    "    #     if model != None:\n",
    "    #         self.model_name = self.__generate_name()\n",
    "    #         assert model != None\n",
    "    #         return model\n",
    "    #     else:\n",
    "    #         assert history != None\n",
    "    #         assert name != None\n",
    "    #         self.model_name = name\n",
    "    #         # model load\n",
    "    #         loadboostrap = loadWriteModel(history)\n",
    "    #         model, _ = loadboostrap.load(name, weight_load=True)\n",
    "    #         return model\n",
    "\n",
    "    def dir_paths(self, path):\n",
    "        \"\"\"\n",
    "        get the validate directory full path from val_path\n",
    "        :param path:\n",
    "        :return: directory name and directory path, which has been sorted\n",
    "        \"\"\"\n",
    "        dir_array = []\n",
    "        for f in listdir(path):\n",
    "            total_path = join(path, f)\n",
    "            if isdir(total_path):\n",
    "                dir_array.append(total_path)\n",
    "        dir_path_array = sorted(dir_array)\n",
    "        labels = [os.path.split(d)[-1] for d in dir_path_array]\n",
    "        return labels, dir_path_array\n",
    "\n",
    "    def __generate_array(self, path):\n",
    "        \"\"\"\n",
    "        read each image file and save them to a numpy array\n",
    "        :param path:\n",
    "        :return: image_arrays, predict_list\n",
    "        \"\"\"\n",
    "        image_arrays = None\n",
    "        predict_list = []\n",
    "        if os.path.isdir(path):\n",
    "            file_list = [join(path, f) for f in listdir(path)]\n",
    "        else:\n",
    "            file_list = [path]\n",
    "\n",
    "        for absolute_path in file_list:\n",
    "            if self.__check_extension(absolute_path):\n",
    "                predict_list.append(absolute_path)\n",
    "                image = load_img(absolute_path)\n",
    "                image = image.resize(self.image_shape)\n",
    "                # load all image to numpy and convert to (None, 224, 224, 3)\n",
    "                image = img_to_array(image)\n",
    "                image = image[np.newaxis, :]\n",
    "                try:\n",
    "                    image_arrays = np.concatenate((image_arrays, image), 0)\n",
    "                except:\n",
    "                    image_arrays = image\n",
    "\n",
    "        if len(predict_list) == 0:\n",
    "            raise RuntimeError(\n",
    "                \"No image find in predit folder path, Please check the file \"\n",
    "                \"extension is \" + self.default_imagefile_extension)\n",
    "\n",
    "        logging.info('Stack image array size: %s', image_arrays.shape)\n",
    "        return image_arrays, predict_list\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_name():\n",
    "        now = datetime.datetime.now()\n",
    "        timestamp = now.strftime('%Y%m%d_%H_%M_%S')\n",
    "        name = timestamp + '_%02d' % (np.random.randint(100))\n",
    "        return name\n",
    "\n",
    "    def __predict(self, image_arrays, predict_list, target_index,\n",
    "                  save_image=True):\n",
    "        \"\"\"\n",
    "        predict, and get result which is a softmax array, and get the max result to compare\n",
    "            the target one\n",
    "        :param predict_list:    the predict file list\n",
    "        :param target_index:    the image which belong to his category\n",
    "        :param save_image:      save image which is predict wrong\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        result = self.model.predict(image_arrays)\n",
    "\n",
    "        # result summary csv row\n",
    "        for i in range(len(result)):\n",
    "            real_index = np.argmax(result[i])\n",
    "            row = {}\n",
    "            wrong_file = predict_list[i]\n",
    "            row['file'] = wrong_file\n",
    "            row['t_index'] = int(target_index)\n",
    "            row['t_label'] = self.labels[target_index]\n",
    "            row['r_index'] = int(real_index)\n",
    "            row['r_label'] = self.labels[real_index]\n",
    "            name = MyPredict.generate_name()\n",
    "            row['timestamp'] = name[:-3]\n",
    "            for index, label in enumerate(self.labels):\n",
    "                key = 'l_' + label\n",
    "                row[key] = str(np.round(result[i][index], 3))\n",
    "\n",
    "            if real_index != target_index:\n",
    "                # predict wrong\n",
    "                row['check'] = 0\n",
    "                logging.debug('Predict:\\n%s', row)\n",
    "\n",
    "                # TODO 保存文件,用统一Flag\n",
    "                if save_image:\n",
    "                    row['image'] = self.__save_image(image_arrays[i], result[i],\n",
    "                                                     wrong_file, name)\n",
    "            else:\n",
    "                row['check'] = 1\n",
    "\n",
    "            self.csv = self.csv.append(row, ignore_index=True)\n",
    "\n",
    "        logging.info('One Folder image predict finish')\n",
    "\n",
    "    def __save_image(self, img_array, result, title, file_name):\n",
    "        \"\"\"\n",
    "        save image\n",
    "        :param img_array:   image array\n",
    "        :param result:      softmax rate\n",
    "        :param title:       image title\n",
    "        :param file_name:   image file name without extension\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        def auto_label(rects, aix):\n",
    "            # Get y-axis height to calculate label position from.\n",
    "            (y_bottom, y_top) = aix.get_ylim()\n",
    "            y_height = y_top - y_bottom\n",
    "\n",
    "            for rect in rects:\n",
    "                height = rect.get_height()\n",
    "\n",
    "                # Fraction of axis height taken up by this rectangle\n",
    "                p_height = (height / y_height)\n",
    "\n",
    "                # If we can fit the label above the column, do that;\n",
    "                # otherwise, put it inside the column.\n",
    "                if p_height > 0.95:  # arbitrary; 95% looked good to me.\n",
    "                    label_position = height - (y_height * 0.05)\n",
    "                else:\n",
    "                    label_position = height + (y_height * 0.01)\n",
    "                height = '%s' % (np.round(height, 2))\n",
    "\n",
    "                aix.text(rect.get_x() + rect.get_width() / 2., label_position,\n",
    "                         height, ha='center', va='bottom', fontsize=6)\n",
    "\n",
    "\n",
    "        labels = self.labels\n",
    "\n",
    "        plt.figure()\n",
    "        image = array_to_img(img_array)\n",
    "        plt.subplot(211)\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)\n",
    "\n",
    "        # TODO 每个bar都有不同颜色\n",
    "        ax = plt.subplot(212)\n",
    "        label_size = len(result)\n",
    "        xticks = np.arange(label_size)\n",
    "        bar = plt.bar(xticks, result, align='center', width=0.9)\n",
    "\n",
    "        # avoid the label size too large\n",
    "        if label_size > 20:\n",
    "            labels = [i[:3] for i in labels]\n",
    "            rotation = 'vertical'\n",
    "            fontsize = 8\n",
    "        else:\n",
    "            labels = tuple(labels)\n",
    "            rotation = 30\n",
    "            fontsize = 6\n",
    "        plt.xticks(xticks, labels, fontsize=fontsize, rotation=rotation)\n",
    "        auto_label(aix=ax, rects=bar)\n",
    "\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        save_path = join(self.mother_path, file_name + '.jpg')\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        return save_path\n",
    "\n",
    "    def __check_extension(self, path):\n",
    "        assert os.path.exists(path), 'Path %s is not exit !' % path\n",
    "        if path.endswith(os.sep):\n",
    "            path = path[:-1]\n",
    "        f = os.path.split(path)[1]\n",
    "        if re.match(r'([\\w]+\\.(?:' + self.default_imagefile_extension + '))',\n",
    "                    f):\n",
    "            logging.debug('Check: %s, %s', \"True\", path)\n",
    "            return True\n",
    "        else:\n",
    "            logging.debug('Check: %s, %s', \"False\", path)\n",
    "            return False\n",
    "\n",
    "    def run(self, pred_path, predict_type, pred_filename=None, ext=None,\n",
    "            confusion_matrix=True, save_predict_image=True,\n",
    "            pandas_format=False):\n",
    "\n",
    "        if ext is not None and ext != '':\n",
    "            self.default_imagefile_extension = ext\n",
    "\n",
    "        assert os.path.exists(pred_path), \"Predict Path {} is not exist\"%(predict_path)\n",
    "\n",
    "\n",
    "        # just one image\n",
    "        if predict_type == MyPredict.NORMAL:\n",
    "            image_arrays, predict_list = self.__generate_array(pred_path)\n",
    "            results = self.model.predict(image_arrays)\n",
    "\n",
    "            # without label use [0,.....n] instead\n",
    "            self.label = self.labels if self.labels != None else [str(index) for\n",
    "                                                                  index in\n",
    "                                                                  np.arange(0,\n",
    "                                                                            len(\n",
    "                                                                                results[\n",
    "                                                                                    0]))]\n",
    "\n",
    "            df = None\n",
    "            if pandas_format:\n",
    "                columns = self.label + ['index']\n",
    "                df = pd.DataFrame(columns=columns)\n",
    "            for index, result in enumerate(results):\n",
    "                np.set_printoptions(linewidth=500, suppress=True)\n",
    "                print('MESSAGE:%s, \\t\\n%s' % (predict_list[index], result))\n",
    "\n",
    "                if pandas_format:\n",
    "                    result_list = list(result.copy())\n",
    "                    result_list.append(str(predict_list[index]))\n",
    "                    df = df.append({key: value for key, value in\n",
    "                                    zip(df.columns.values, result_list)},\n",
    "                                   ignore_index=True)\n",
    "\n",
    "                if save_predict_image:\n",
    "                    self.__save_image(image_arrays[0], result, pred_path,\n",
    "                                      MyPredict.generate_name())\n",
    "\n",
    "\n",
    "\n",
    "            if pandas_format:\n",
    "                return df\n",
    "\n",
    "        # one folder has many folder each folder's name is label\n",
    "        elif predict_type == MyPredict.CATEGORY:\n",
    "            labels, dir_paths = self.dir_paths(pred_path)\n",
    "            self.labels = labels\n",
    "            logging.debug('labels: %s, \\ndir_paths: %s', labels, dir_paths)\n",
    "\n",
    "            # log\n",
    "            if pred_filename is None or pred_filename == '':\n",
    "                predict_logfile_name = self.model.name + self.default_pred_filename_ext\n",
    "            else:\n",
    "                assert pred_filename.endswith('.csv')\n",
    "                predict_logfile_name = pred_filename\n",
    "\n",
    "            self.__init_csv()\n",
    "\n",
    "            for i, path in enumerate(dir_paths):\n",
    "                logging.info('Program read next directory: %s', path)\n",
    "                # read every directory's image file and generate image array\n",
    "                images, predict_list = self.__generate_array(path)\n",
    "                self.__predict(images, predict_list, i, save_image=save_predict_image)\n",
    "\n",
    "            # csv save\n",
    "            self.csv.to_csv(join(self.mother_path, predict_logfile_name),\n",
    "                            index=False)\n",
    "\n",
    "\n",
    "            logging.info('Logging Saved')\n",
    "\n",
    "            if confusion_matrix:\n",
    "                self.__generate_confusion_matrix()\n",
    "\n",
    "            if pandas_format:\n",
    "                return self.csv\n",
    "\n",
    "        # TODO 预测准确度\n",
    "        else:\n",
    "            raise RuntimeError(\"Predict type must be NORMAL or CATEGORY\")\n",
    "\n",
    "    def __generate_confusion_matrix(self):\n",
    "        matrix = np.zeros((len(self.labels),) * 2)\n",
    "        column_name = ['r_index', 't_index']\n",
    "        search_csv = self.csv[column_name]\n",
    "        np.set_printoptions(linewidth=500)\n",
    "        for index, row in search_csv.iterrows():\n",
    "            i, j = row[column_name[0]], row[column_name[1]]\n",
    "            matrix[int(i)][int(j)] += 1\n",
    "\n",
    "        confus_matrix = pd.DataFrame(matrix, index=self.labels,\n",
    "                                     columns=self.labels, dtype='int64')\n",
    "        pd.set_option('display.width', 500)\n",
    "        logging.info(\"MESSAGE: \\n%s\" % (confus_matrix))\n",
    "\n",
    "        confus_matrix.to_csv(self.default_confusion_matrix_path)\n",
    "        logging.info('Confusion Matrix Saved')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model_path = './weight/cifa.json'\n",
    "    weight_path = './weight/cifa.h5'\n",
    "    predict_path = './cifa'\n",
    "    # predict_path = '/Users/sober/Workspace/Python/PredictTools/Image/0491.jpg'\n",
    "    model = MyPredict.read_file_model(model_path, weight_path,\n",
    "                                      show_summary=True)\n",
    "#     model = MyPredict.read_log_model(name='xx', history_path='history')\n",
    "    predict = MyPredict(model=model, level=[logging.INFO], mother_path='xx')\n",
    "    predict.run(pred_filename='time.csv', predict_type=MyPredict.CATEGORY,\n",
    "                pred_path=predict_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from matplotlib.pyplot import imshow\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''\n",
    "\n",
    "def reformat(input):\n",
    "    output = []\n",
    "    print(type(input))\n",
    "    for i in input:\n",
    "       output.append('%.3f' %i)\n",
    "    return output\n",
    "\n",
    "model = load_model('weight/flower_gpu.h5')\n",
    "filepath = '/home/sober/ftp_bk/Dataset/Tmp_Train/flo/'\n",
    "for d in os.listdir(filepath):\n",
    "    dirpath = filepath + d + '/'\n",
    "    for f in os.listdir(dirpath):\n",
    "        file = dirpath + f\n",
    "        print(file)\n",
    "        im = Image.open(file)\n",
    "        im = im.resize((150, 150), Image.ANTIALIAS)\n",
    "        im = np.asarray(im)\n",
    "        im = im[np.newaxis, :]\n",
    "#         imshow(np.asarray(im))\n",
    "#         break\n",
    "        print(np.array2string(model.predict(np.asarray(im)), formatter={'float_kind':lambda x: \"%.2f\" % x}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFLite file Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/tulips/10128546863_8de70c610d.jpg\n",
      "[[0.10 0.11 0.10 0.09 0.07 0.09 0.14 0.11 0.11 0.08]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/tulips/10686568196_b1915544a8.jpg\n",
      "[[0.43 0.13 0.01 0.08 0.06 0.11 0.04 0.04 0.01 0.09]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/sunflowers/10386540106_1431e73086_m.jpg\n",
      "[[0.04 0.12 0.08 0.11 0.12 0.12 0.12 0.10 0.08 0.12]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/sunflowers/10386522775_4f8c616999_m.jpg\n",
      "[[0.03 0.10 0.09 0.04 0.11 0.09 0.18 0.16 0.08 0.12]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/sunflowers/1022552002_2b93faf9e7_n.jpg\n",
      "[[0.07 0.09 0.15 0.11 0.11 0.07 0.10 0.12 0.06 0.10]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/sunflowers/10386503264_e05387e1f7_m.jpg\n",
      "[[0.05 0.09 0.11 0.12 0.16 0.16 0.11 0.12 0.04 0.04]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/sunflowers/10386525695_2c38fea555_n.jpg\n",
      "[[0.10 0.11 0.12 0.11 0.08 0.13 0.12 0.11 0.06 0.07]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/sunflowers/1022552036_67d33d5bd8_n.jpg\n",
      "[[0.04 0.12 0.10 0.10 0.12 0.12 0.10 0.12 0.08 0.08]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/sunflowers/10386525005_fd0b7d6c55_n.jpg\n",
      "[[0.06 0.07 0.13 0.11 0.10 0.13 0.17 0.14 0.03 0.05]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/daisy/100080576_f52e8ee070_n.jpg\n",
      "[[0.11 0.10 0.09 0.11 0.09 0.10 0.10 0.09 0.10 0.10]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/daisy/1031799732_e7f4008c03.jpg\n",
      "[[0.07 0.09 0.11 0.09 0.14 0.09 0.11 0.11 0.08 0.12]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/12045735155_42547ce4e9_n.jpg\n",
      "[[0.10 0.14 0.11 0.11 0.11 0.10 0.09 0.11 0.07 0.07]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/11233672494_d8bf0a3dbf_n.jpg\n",
      "[[0.16 0.08 0.02 0.03 0.00 0.10 0.03 0.08 0.03 0.46]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/12240577184_b0de0e53ea_n.jpg\n",
      "[[0.04 0.09 0.08 0.11 0.10 0.17 0.15 0.11 0.08 0.06]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/11694025703_9a906fedc1_n.jpg\n",
      "[[0.06 0.07 0.10 0.11 0.14 0.08 0.14 0.14 0.09 0.07]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/10503217854_e66a804309.jpg\n",
      "[[0.02 0.03 0.01 0.09 0.03 0.21 0.48 0.06 0.02 0.05]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/12202373204_34fb07205b.jpg\n",
      "[[0.06 0.10 0.10 0.07 0.10 0.13 0.17 0.11 0.07 0.09]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/10894627425_ec76bbc757_n.jpg\n",
      "[[0.08 0.09 0.09 0.07 0.09 0.09 0.23 0.11 0.06 0.08]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/12165480946_c4a3fe182d_n.jpg\n",
      "[[0.07 0.12 0.08 0.10 0.09 0.12 0.11 0.12 0.08 0.11]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/118974357_0faa23cce9_n.jpg\n",
      "[[0.03 0.08 0.06 0.06 0.10 0.21 0.28 0.04 0.03 0.12]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/12238827553_cf427bfd51_n.jpg\n",
      "[[0.05 0.06 0.08 0.11 0.08 0.14 0.12 0.21 0.03 0.11]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/12240165555_98625b1e88_n.jpg\n",
      "[[0.04 0.08 0.06 0.14 0.06 0.06 0.11 0.31 0.05 0.10]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/11944957684_2cc806276e.jpg\n",
      "[[0.05 0.13 0.06 0.10 0.18 0.16 0.07 0.07 0.12 0.06]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/12323085443_8ac0cdb713_n.jpg\n",
      "[[0.08 0.10 0.06 0.08 0.08 0.10 0.23 0.11 0.04 0.13]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/11102341464_508d558dfc_n.jpg\n",
      "[[0.07 0.10 0.10 0.09 0.13 0.10 0.12 0.11 0.09 0.09]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/12243068283_ee4c2683e2_n.jpg\n",
      "[[0.08 0.06 0.09 0.07 0.13 0.11 0.19 0.12 0.05 0.09]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/110472418_87b6a3aa98_m.jpg\n",
      "[[0.21 0.07 0.02 0.09 0.01 0.18 0.22 0.13 0.01 0.05]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/123128873_546b8b7355_n.jpg\n",
      "[[0.07 0.09 0.10 0.14 0.10 0.09 0.10 0.20 0.03 0.08]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/12243069253_e512464095_n.jpg\n",
      "[[0.06 0.06 0.10 0.07 0.12 0.11 0.16 0.19 0.06 0.06]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/roses/12240303_80d87f77a3_n.jpg\n",
      "[[0.06 0.07 0.11 0.08 0.15 0.13 0.12 0.13 0.08 0.07]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/dandelion/10779476016_9130714dc0.jpg\n",
      "[[0.10 0.12 0.09 0.11 0.10 0.11 0.08 0.11 0.09 0.09]]\n",
      "/home/sober/ftp_bk/Dataset/Tmp_Train/flo/dandelion/11124381625_24b17662bd_n.jpg\n",
      "[[0.03 0.06 0.07 0.08 0.10 0.10 0.34 0.15 0.03 0.05]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'daisy dandelion roses sunflowers tulips'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import os\n",
    "filepath='./TF_Flower/cifa.tflite'\n",
    "# filepath='./weight/flower5.lite'\n",
    "val_path = '/home/sober/ftp_bk/Dataset/Tmp_Train/flo/'\n",
    "shape=(32, 32)\n",
    "\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.contrib.lite.Interpreter(model_path=filepath)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "# change the following line to feed into your own data.\n",
    "\n",
    "for d in os.listdir(val_path):\n",
    "    dirpath = val_path+ d + '/'\n",
    "    for f in os.listdir(dirpath):\n",
    "        file = dirpath + f\n",
    "        print(file)\n",
    "        im = Image.open(file)\n",
    "        im = im.resize(shape, Image.ANTIALIAS)\n",
    "        im = np.asarray(im)\n",
    "        im = im[np.newaxis, :]\n",
    "        im = np.asarray(im)\n",
    "#         print(np.array2string(model.predict(np.asarray(im)), formatter={'float_kind':lambda x: \"%.2f\" % x}))\n",
    "        input_data = np.array(im, dtype=np.float32)\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "#         output_data.sort()\n",
    "        print(np.array2string(output_data, formatter={'float_kind':lambda x: \"%.2f\" % x}))\n",
    "    \n",
    "\"daisy dandelion roses sunflowers tulips\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow .pb, .tflite, get input and output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input :['input_1']\n",
      "Output: ['fc1000/Softmax']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.platform import gfile\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "import os\n",
    "\n",
    "def save_graph_to_file(sess,  graph_file_name, output_names):\n",
    "    output_graph_def = graph_util.convert_variables_to_constants(\n",
    "      sess,  sess.graph.as_graph_def(),  output_names)\n",
    "    with gfile.FastGFile(graph_file_name, 'wb') as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "def keras2tflite(sess, input_tensors, output_tensors, filename):\n",
    "    '''\n",
    "    input_tensor and output_tensors is array and is keras input and output tensor\n",
    "    filename must be .tflite\n",
    "    '''\n",
    "    converter = tf.contrib.lite.TocoConverter.from_session(sess, input_tensors, output_tensors)\n",
    "    tflite_model=converter.convert()\n",
    "    open(filename, \"wb\").write(tflite_model)\n",
    "    \n",
    "def get_input_output_layer(filename) :\n",
    "    \"\"\"\n",
    "    It must clean the gpu memory, otherwise the input and output name is incorrect !\n",
    "    \"\"\"\n",
    "    if 'pb' in filename:\n",
    "        gf = tf.GraphDef()\n",
    "        gf.ParseFromString(open(filename,'rb').read())\n",
    "        for n in gf.node:\n",
    "            print(n.name + ' =>> ' + n.op )\n",
    "    elif 'h5' in filename:\n",
    "        assert(os.path.exists(filename))\n",
    "        model = load_model(filename)\n",
    "        input_names = [node.op.name for node in model.inputs]\n",
    "        output_names = [node.op.name for node in model.outputs]\n",
    "        print('Input :'  + str(input_names) + '\\nOutput: ' +  str(output_names))\n",
    "        \n",
    "get_input_output_layer('./weight/cifa.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Self-build Model seems Cat/Dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import os\n",
    "from keras import optimizers\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "train_path='/home/sober/code/jupyter/ftp_code/AndroidTensorflow/tensorflow-for-poets-2/tf_files/flower_photos'\n",
    "val_path='/home/sober/code/jupyter/ftp_code/AndroidTensorflow/tensorflow-for-poets-2/tf_files/flower_photos'\n",
    "batch_size = 16\n",
    "epochs = 200\n",
    "\n",
    "input = Input(shape = (150, 150, 3), name = 'input')\n",
    "x = Conv2D(32, (3, 3), activation='relu')(input)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "# x = Dense(64, activation='relu')(x)\n",
    "# x = Dense(2, activation='sigmoid', name = 'output')(x)\n",
    "x = Dense(5, activation='softmax', name = 'output')(x)\n",
    "model = Model(inputs = input, outputs= x)\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.000005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_path,  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        val_path,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=1340 // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        shuffle=True,\n",
    "        validation_steps=64 // batch_size)\n",
    "model.save('./weight/flower5.h5')  # always save your weights after training or during training\n",
    "\n",
    "# convert\n",
    "output_names = [node.op.name for node in model.outputs]\n",
    "\n",
    "export_dir = './weight/'\n",
    "sess = K.get_session()\n",
    "save_graph_to_file(sess,  export_dir + \"flower5.pb\", output_names)\n",
    "\n",
    "\n",
    "# converter = tf.contrib.lite.TocoConverter.from_session(sess, [input], [x])\n",
    "# tflite_model=converter.convert()\n",
    "# open(export_dir+\"flower_gpu.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# frozen_graphdef = tf.graph_util.convert_variables_to_constants(\n",
    "#       sess, sess.graph_def, output_names)\n",
    "# tflite_model = tf.contrib.lite.toco_convert(frozen_graphdef, [input], [x])\n",
    "# tflite_model = tf.contrib.lite.toco_convert(frozen_graphdef, [input], output_names)\n",
    "# open(export_dir+\"flower_gpu.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input_names = [node.op.name for node in model.inputs]\n",
    "\n",
    "# sess = tf.keras.backend.get_session()\n",
    "# frozen_def = tf.graph_util.convert_variables_to_constants(\n",
    "#     sess, sess.graph_def, output_names)\n",
    "\n",
    "# tflite_graph = 'code_cifa.tflite'\n",
    "# tflite_model = tf.contrib.lite.toco_convert(\n",
    "#     frozen_def, input_names, output_names)\n",
    "# with tf.gfile.GFile(tflite_graph, 'wb') as f:\n",
    "#     f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Slim VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\"\n",
    "\n",
    "train_dir = '/home/sober/code/jupyter/ftp_code/AndroidTensorflow/tensorflow-for-poets-2/tf_files/flower_photos'\n",
    "val_dir = '/home/sober/code/jupyter/ftp_code/AndroidTensorflow/tensorflow-for-poets-2/tf_files/flower_photos'\n",
    "model_path = './vgg_16.ckpt'\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "num_epochs1 =  1\n",
    "num_epochs2 = 0\n",
    "learning_rate1 = 1e-3\n",
    "learning_rate2 = 1e-5\n",
    "dropout_keep_prob = 0.5\n",
    "weight_decay = 5e-4\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--train_dir', default='coco-animals/train')\n",
    "# parser.add_argument('--val_dir', default='coco-animals/val')\n",
    "# parser.add_argument('--model_path', default='vgg_16.ckpt', type=str)\n",
    "# parser.add_argument('--batch_size', default=32, type=int)\n",
    "# parser.add_argument('--num_workers', default=4, type=int)\n",
    "# parser.add_argument('--num_epochs1', default=1, type=int)\n",
    "# parser.add_argument('--num_epochs2', default=1, type=int)\n",
    "# parser.add_argument('--learning_rate1', default=1e-3, type=float)\n",
    "# parser.add_argument('--learning_rate2', default=1e-5, type=float)\n",
    "# parser.add_argument('--dropout_keep_prob', default=0.5, type=float)\n",
    "# parser.add_argument('--weight_decay', default=5e-4, type=float)\n",
    "\n",
    "VGG_MEAN = [0.0, 0.0, 0.0]\n",
    "# VGG_MEAN = [123.68, 116.78, 103.94]\n",
    "\n",
    "\n",
    "def list_images(directory):\n",
    "    \"\"\"\n",
    "    Get all the images and labels in directory/label/*.jpg\n",
    "    \"\"\"\n",
    "    labels = os.listdir(directory)\n",
    "    # Sort the labels so that training and validation get them in the same order\n",
    "    labels.sort()\n",
    "\n",
    "    files_and_labels = []\n",
    "    for label in labels:\n",
    "        for f in os.listdir(os.path.join(directory, label)):\n",
    "            files_and_labels.append((os.path.join(directory, label, f), label))\n",
    "\n",
    "    filenames, labels = zip(*files_and_labels)\n",
    "    filenames = list(filenames)\n",
    "    labels = list(labels)\n",
    "    unique_labels = list(set(labels))\n",
    "\n",
    "    label_to_int = {}\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        label_to_int[label] = i\n",
    "\n",
    "    labels = [label_to_int[l] for l in labels]\n",
    "\n",
    "    return filenames, labels\n",
    "\n",
    "\n",
    "def check_accuracy(sess, correct_prediction, is_training, dataset_init_op):\n",
    "    \"\"\"\n",
    "    Check the accuracy of the model on either train or val (depending on dataset_init_op).\n",
    "    \"\"\"\n",
    "    # Initialize the correct dataset\n",
    "    sess.run(dataset_init_op)\n",
    "    num_correct, num_samples = 0, 0\n",
    "    while True:\n",
    "        try:\n",
    "            correct_pred = sess.run(correct_prediction, {is_training: False})\n",
    "            num_correct += correct_pred.sum()\n",
    "            num_samples += correct_pred.shape[0]\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "    # Return the fraction of datapoints that were correctly classified\n",
    "    acc = float(num_correct) / num_samples\n",
    "    return acc\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Get the list of filenames and corresponding list of labels for training et validation\n",
    "    train_filenames, train_labels = list_images(train_dir)\n",
    "    val_filenames, val_labels = list_images(val_dir)\n",
    "    assert set(train_labels) == set(val_labels),\\\n",
    "           \"Train and val labels don't correspond:\\n{}\\n{}\".format(set(train_labels),\n",
    "                                                                   set(val_labels))\n",
    "\n",
    "    num_classes = len(set(train_labels))\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # In TensorFlow, you first want to define the computation graph with all the\n",
    "    # necessary operations: loss, training op, accuracy...\n",
    "    # Any tensor created in the `graph.as_default()` scope will be part of `graph`\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Standard preprocessing for VGG on ImageNet taken from here:\n",
    "        # https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/vgg_preprocessing.py\n",
    "        # Also see the VGG paper for more details: https://arxiv.org/pdf/1409.1556.pdf\n",
    "\n",
    "        # Preprocessing (for both training and validation):\n",
    "        # (1) Decode the image from jpg format\n",
    "        # (2) Resize the image so its smaller side is 256 pixels long\n",
    "        def _parse_function(filename, label):\n",
    "            image_string = tf.read_file(filename)\n",
    "            image_decoded = tf.image.decode_jpeg(image_string, channels=3)          # (1)\n",
    "            image = tf.cast(image_decoded, tf.float32)\n",
    "\n",
    "            smallest_side = 256.0\n",
    "            height, width = tf.shape(image)[0], tf.shape(image)[1]\n",
    "            height = tf.to_float(height)\n",
    "            width = tf.to_float(width)\n",
    "\n",
    "            scale = tf.cond(tf.greater(height, width),\n",
    "                            lambda: smallest_side / width,\n",
    "                            lambda: smallest_side / height)\n",
    "            new_height = tf.to_int32(height * scale)\n",
    "            new_width = tf.to_int32(width * scale)\n",
    "\n",
    "            resized_image = tf.image.resize_images(image, [new_height, new_width])  # (2)\n",
    "            return resized_image, label\n",
    "\n",
    "        # Preprocessing (for training)\n",
    "        # (3) Take a random 224x224 crop to the scaled image\n",
    "        # (4) Horizontally flip the image with probability 1/2\n",
    "        # (5) Substract the per color mean `VGG_MEAN`\n",
    "        # Note: we don't normalize the data here, as VGG was trained without normalization\n",
    "        def training_preprocess(image, label):\n",
    "            crop_image = tf.random_crop(image, [224, 224, 3])                       # (3)\n",
    "            flip_image = tf.image.random_flip_left_right(crop_image)                # (4)\n",
    "\n",
    "            means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
    "            centered_image = flip_image - means                                     # (5)\n",
    "\n",
    "            return centered_image, label\n",
    "\n",
    "        # Preprocessing (for validation)\n",
    "        # (3) Take a central 224x224 crop to the scaled image\n",
    "        # (4) Substract the per color mean `VGG_MEAN`\n",
    "        # Note: we don't normalize the data here, as VGG was trained without normalization\n",
    "        def val_preprocess(image, label):\n",
    "            crop_image = tf.image.resize_image_with_crop_or_pad(image, 224, 224)    # (3)\n",
    "\n",
    "            means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
    "            centered_image = crop_image - means                                     # (4)\n",
    "\n",
    "            return centered_image, label\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # DATASET CREATION using tf.contrib.data.Dataset\n",
    "        # https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data\n",
    "\n",
    "        # The tf.contrib.data.Dataset framework uses queues in the background to feed in\n",
    "        # data to the model.\n",
    "        # We initialize the dataset with a list of filenames and labels, and then apply\n",
    "        # the preprocessing functions described above.\n",
    "        # Behind the scenes, queues will load the filenames, preprocess them with multiple\n",
    "        # threads and apply the preprocessing in parallel, and then batch the data\n",
    "\n",
    "        # Training dataset\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((train_filenames, train_labels))\n",
    "        train_dataset = train_dataset.map(_parse_function,\n",
    "            num_parallel_calls=num_workers)\n",
    "        train_dataset = train_dataset.map(training_preprocess,\n",
    "            num_parallel_calls=num_workers)\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=10000)  # don't forget to shuffle\n",
    "        batched_train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "        # Validation dataset\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((val_filenames, val_labels))\n",
    "        val_dataset = val_dataset.map(_parse_function,\n",
    "            num_parallel_calls=num_workers)\n",
    "        val_dataset = val_dataset.map(val_preprocess,\n",
    "            num_parallel_calls=num_workers)\n",
    "        batched_val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "        # Now we define an iterator that can operator on either dataset.\n",
    "        # The iterator can be reinitialized by calling:\n",
    "        #     - sess.run(train_init_op) for 1 epoch on the training set\n",
    "        #     - sess.run(val_init_op)   for 1 epoch on the valiation set\n",
    "        # Once this is done, we don't need to feed any value for images and labels\n",
    "        # as they are automatically pulled out from the iterator queues.\n",
    "\n",
    "        # A reinitializable iterator is defined by its structure. We could use the\n",
    "        # `output_types` and `output_shapes` properties of either `train_dataset`\n",
    "        # or `validation_dataset` here, because they are compatible.\n",
    "        iterator = tf.data.Iterator.from_structure(batched_train_dataset.output_types,\n",
    "                                                           batched_train_dataset.output_shapes)\n",
    "        images, labels = iterator.get_next()\n",
    "#         input tensor\n",
    "        print(type(images), images.name)\n",
    "        \n",
    "        train_init_op = iterator.make_initializer(batched_train_dataset)\n",
    "        val_init_op = iterator.make_initializer(batched_val_dataset)\n",
    "\n",
    "        # Indicates whether we are in training or in test mode\n",
    "        is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Now that we have set up the data, it's time to set up the model.\n",
    "        # For this example, we'll use VGG-16 pretrained on ImageNet. We will remove the\n",
    "        # last fully connected layer (fc8) and replace it with our own, with an\n",
    "        # output size num_classes=8\n",
    "        # We will first train the last layer for a few epochs.\n",
    "        # Then we will train the entire model on our dataset for a few epochs.\n",
    "\n",
    "        # Get the pretrained model, specifying the num_classes argument to create a new\n",
    "        # fully connected replacing the last one, called \"vgg_16/fc8\"\n",
    "        # Each model has a different architecture, so \"vgg_16/fc8\" will change in another model.\n",
    "        # Here, logits gives us directly the predicted scores we wanted from the images.\n",
    "        # We pass a scope to initialize \"vgg_16/fc8\" weights with he_initializer\n",
    "        vgg = tf.contrib.slim.nets.vgg\n",
    "        with slim.arg_scope(vgg.vgg_arg_scope(weight_decay=weight_decay)):\n",
    "            logits, end_points = vgg.vgg_16(images, num_classes=num_classes, is_training=is_training,\n",
    "                                   dropout_keep_prob=dropout_keep_prob)\n",
    "\n",
    "        # Specify where the model checkpoint is (pretrained weights).\n",
    "#         model_path = model_path\n",
    "        assert(os.path.isfile(model_path))\n",
    "\n",
    "        # Restore only the layers up to fc7 (included)\n",
    "        # Calling function `init_fn(sess)` will load all the pretrained weights.\n",
    "        variables_to_restore = tf.contrib.framework.get_variables_to_restore(exclude=['vgg_16/fc8'])\n",
    "        init_fn = tf.contrib.framework.assign_from_checkpoint_fn(model_path, variables_to_restore)\n",
    "\n",
    "        # Initialization operation from scratch for the new \"fc8\" layers\n",
    "        # `get_variables` will only return the variables whose name starts with the given pattern\n",
    "        fc8_variables = tf.contrib.framework.get_variables('vgg_16/fc8')\n",
    "        fc8_init = tf.variables_initializer(fc8_variables)\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Using tf.losses, any loss is added to the tf.GraphKeys.LOSSES collection\n",
    "        # We can then call the total loss easily\n",
    "        tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "        loss = tf.losses.get_total_loss()\n",
    "\n",
    "        # First we want to train only the reinitialized last layer fc8 for a few epochs.\n",
    "        # We run minimize the loss only with respect to the fc8 variables (weight and bias).\n",
    "        fc8_optimizer = tf.train.GradientDescentOptimizer(learning_rate1)\n",
    "        fc8_train_op = fc8_optimizer.minimize(loss, var_list=fc8_variables)\n",
    "\n",
    "        # Then we want to finetune the entire model for a few epochs.\n",
    "        # We run minimize the loss only with respect to all the variables.\n",
    "        full_optimizer = tf.train.GradientDescentOptimizer(learning_rate2)\n",
    "        full_train_op = full_optimizer.minimize(loss)\n",
    "\n",
    "        # Evaluation metrics\n",
    "        prediction = tf.to_int32(tf.argmax(logits, 1))\n",
    "        correct_prediction = tf.equal(prediction, labels)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        tf.get_default_graph().finalize()\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Now that we have built the graph and finalized it, we define the session.\n",
    "    # The session is the interface to *run* the computational graph.\n",
    "    # We can call our training operations with `sess.run(train_op)` for instance\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        init_fn(sess)  # load the pretrained weights\n",
    "        sess.run(fc8_init)  # initialize the new fc8 layer\n",
    "\n",
    "        # Update only the last layer for a few epochs.\n",
    "        for epoch in range(num_epochs1):\n",
    "            # Run an epoch over the training data.\n",
    "            print('Starting epoch %d / %d' % (epoch + 1, num_epochs1))\n",
    "            # Here we initialize the iterator with the training set.\n",
    "            # This means that we can go through an entire epoch until the iterator becomes empty.\n",
    "            sess.run(train_init_op)\n",
    "            while True:\n",
    "                try:\n",
    "                    _ = sess.run(fc8_train_op, {is_training: True})\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "\n",
    "            # Check accuracy on the train and val sets every epoch.\n",
    "            train_acc = check_accuracy(sess, correct_prediction, is_training, train_init_op)\n",
    "            val_acc = check_accuracy(sess, correct_prediction, is_training, val_init_op)\n",
    "            print('Train accuracy: %f' % train_acc)\n",
    "            print('Val accuracy: %f\\n' % val_acc)\n",
    "\n",
    "\n",
    "            \n",
    "        # Train the entire model for a few more epochs, continuing with the *same* weights.\n",
    "        for epoch in range(num_epochs2):\n",
    "            print('Starting epoch %d / %d' % (epoch + 1, num_epochs2))\n",
    "            sess.run(train_init_op)\n",
    "            while True:\n",
    "                try:\n",
    "                    _ = sess.run(full_train_op, {is_training: True})\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "\n",
    "            # Check accuracy on the train and val sets every epoch\n",
    "            train_acc = check_accuracy(sess, correct_prediction, is_training, train_init_op)\n",
    "            val_acc = check_accuracy(sess, correct_prediction, is_training, val_init_op)\n",
    "            print('Train accuracy: %f' % train_acc)\n",
    "            print('Val accuracy: %f\\n' % val_acc)\n",
    "        \n",
    "        converter = tf.contrib.lite.TocoConverter.from_session(sess, input_tensors, output_tensors)\n",
    "        tflite_model=converter.convert()\n",
    "        open(\"./weight/tf_flower5.tflite\", \"wb\").write(tflite_model)\n",
    "        \n",
    "        gf = tf.get_default_graph().as_graph_def().node\n",
    "        for g in gf:\n",
    "            print(g.name)\n",
    "        fc8_feature = end_points['vgg_16/fc8']\n",
    "        print(type(fc8_feature), fc8_feature.name)\n",
    "\n",
    "        save_graph_to_file(sess,  \"./weight/tf_flower5.pb\",  [\"vgg_16/fc8/squeezed\"])\n",
    "        \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.6.2",
   "language": "python",
   "name": "3.6.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
